[
    {
        "id": 1,
        "questions": [
            "A company has a workload that requires 14,000 consistent IOPS for data that must be durable and secure. The compliance standards of the company state that the data should be secure at every stage of its lifecycle on all of the EBS volumes they use.",
            "Which of the following statements are true regarding data security on EBS?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "EBS volumes support both in-flight encryption and encryption at rest using KMS",
                "check": true
            },
            {
                "answer": "EBS volumes don't support any encryption",
                "check": false
            },
            {
                "answer": "EBS volumes do not support in-flight encryption but do support encryption at rest using KMS",
                "check": false
            },
            {
                "answer": "EBS volumes support in-flight encryption but does not support encryption at rest",
                "check": false
            }
        ],
        "explanation": {
            "correct": "Amazon EBS works with AWS KMS to encrypt and decrypt your EBS volume. You can encrypt both the boot and data volumes of an EC2 instance. When you create an encrypted EBS volume and\nattach it to a supported instance type, the following types of data are encrypted:Data at rest inside the volumeAll data moving between the volume and the instanceAll snapshots created from the volumeAll volumes created from those snapshots<strong>EBS volumes support both in-flight encryption and encryption at rest using KMS</strong> - This is a correct statement. Encryption operations occur on the servers\nthat host EC2 instances, ensuring the security of both data-at-rest and data-in-transit\nbetween an instance and its attached EBS storage.",
            "incorrect": "<strong>EBS volumes support in-flight encryption but do not support encryption at rest</strong> - This is an incorrect statement. As discussed above, all data moving\nbetween the volume and the instance is encrypted.<strong>EBS volumes do not support in-flight encryption but do support encryption at rest using KMS</strong> - This is an incorrect statement. As discussed above, data at rest is\nalso encrypted.<strong>EBS volumes don't support any encryption</strong> - This is an incorrect statement. Amazon EBS encryption offers a straight-forward encryption solution for your EBS resources\nassociated with your EC2 instances. With Amazon EBS encryption, you aren't required to\nbuild, maintain, and secure your own key management infrastructure.Reference:",
            "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html"
        }
    },
    {
        "id": 2,
        "questions": [
            "After a test deployment in ElasticBeanstalk environment, a developer noticed that all accumulated Amazon EC2 burst balances were lost.",
            "Which of the following options can lead to this behavior?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances",
                "check": false
            },
            {
                "answer": "The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances",
                "check": false
            },
            {
                "answer": "When a canary deployment fails, it resets the EC2 burst balances to zero",
                "check": false
            },
            {
                "answer": "The deployment was either run with immutable updates or in traffic splitting mode",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>The deployment was either run with immutable updates or in traffic splitting mode</strong> - Immutable deployments perform an immutable update to launch a full set\nof new instances running the new version of the application in a separate Auto Scaling\ngroup, alongside the instances running the old version. Immutable deployments can prevent\nissues caused by partially completed rolling deployments.Traffic-splitting deployments let you perform canary testing as part of your application deployment. In a traffic-splitting deployment, Elastic Beanstalk launches a full set of new\ninstances just like during an immutable deployment. It then forwards a specified percentage\nof incoming client traffic to the new application version for a specified evaluation period.Some policies replace all instances during the deployment or update. This causes all accumulated Amazon EC2 burst balances to be lost. It happens in the following cases:",
            "incorrect": "<strong>The deployment was run as a Rolling deployment, resulting in the resetting of EC2 burst balances</strong> - With rolling deployments, Elastic Beanstalk splits the\nenvironment's Amazon EC2 instances into batches and deploys the new version of the\napplication to one batch at a time. Rolling deployments do not result in loss of EC2 burst\nbalances.<strong>The deployment was run as a All-at-once deployment, flushing all the accumulated EC2 burst balances</strong> - The traditional All-at-once deployment, wherein all the\ninstances are updated simultaneously, does not result in loss of EC2 burst balances.<strong>When a canary deployment fails, it resets the EC2 burst balances to zero</strong> - This is incorrect and given only as a distractor.Reference:",
            "reference": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html"
        }
    },
    {
        "id": 3,
        "questions": [
            "As a developer, you are working on creating an application using AWS Cloud Development Kit (CDK).",
            "Which of the following represents the correct order of steps to be followed for creating an app using AWS CDK?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Create the app from a template provided by AWS CDK -> Add code to the app to create resources within stacks -> Build the app (optional) -> Synthesize one or more\nstacks in the app -> Deploy stack(s) to your AWS account",
                "check": true
            },
            {
                "answer": "Create the app from a template provided by AWS CDK -> Add code to the app to create resources within stacks -> Synthesize one or more stacks in the app -> Deploy\nstack(s) to your AWS account -> Build the app",
                "check": false
            },
            {
                "answer": "Create the app from a template provided by AWS CloudFormation -> Add code to the app to create resources within stacks -> Synthesize one or more stacks in the app ->\nDeploy stack(s) to your AWS account -> Build the app",
                "check": false
            },
            {
                "answer": "Create the app from a template provided by AWS CloudFormation -> Add code to the app to create resources within stacks -> Build the app (optional) -> Synthesize one or\nmore stacks in the app -> Deploy stack(s) to your AWS account",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize one or\nmore stacks in the app -&gt; Deploy stack(s) to your AWS account</strong>The standard AWS CDK development workflow is similar to the workflow you're already familiar as a developer. There are a few extra steps:Create the app from a template provided by AWS CDK - Each AWS CDK app should be in its own directory, with its own local module dependencies. Create a new directory\nfor your app. Now initialize the app using the <code>cdk init</code> command,\nspecifying the desired template (\"app\") and programming language. The\n<code>cdk init</code> command creates a number of files and folders inside the\ncreated home directory to help you organize the source code for your AWS CDK app.Add code to the app to create resources within stacks - Add custom code as is needed for your application.Build the app (optional) - In most programming environments, after making changes to your code, you'd build (compile) it. This isn't strictly necessary with the AWS\nCDK—the Toolkit does it for you so you can't forget. But you can still build\nmanually whenever you want to catch syntax and type errors.Synthesize one or more stacks in the app to create an AWS CloudFormation template - Synthesize one or more stacks in the app to create an AWS CloudFormation template.\nThe synthesis step catches logical errors in defining your AWS resources. If your\napp contains more than one stack, you'd need to specify which stack(s) to\nsynthesize.Deploy one or more stacks to your AWS account - It is optional (though good practice) to synthesize before deploying. The AWS CDK synthesizes your stack before each\ndeployment. If your code has security implications, you'll see a summary of these\nand need to confirm them before deployment proceeds. <code>cdk deploy</code> is used\nto deploy the stack using CloudFormation templates. This command displays progress\ninformation as your stack is deployed. When it's done, the command prompt reappears.",
            "incorrect": "<strong>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Build the app (optional) -&gt; Synthesize\none or more stacks in the app -&gt; Deploy stack(s) to your AWS account</strong><strong>Create the app from a template provided by AWS CloudFormation -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app\n-&gt; Deploy stack(s) to your AWS account -&gt; Build the app</strong>For both these options, you cannot use AWS CloudFormation to create the app. So these options are incorrect.<strong>Create the app from a template provided by AWS CDK -&gt; Add code to the app to create resources within stacks -&gt; Synthesize one or more stacks in the app -&gt;\nDeploy stack(s) to your AWS account -&gt; Build the app</strong> - You cannot have the\nbuild step after deployment. So this option is incorrect.Reference:",
            "reference": "https://docs.aws.amazon.com/cdk/latest/guide/hello_world.html"
        }
    },
    {
        "id": 4,
        "questions": [
            "You work as a developer doing contract work for the government on AWS gov cloud. Your applications use Amazon Simple Queue Service (SQS) for its message queue service. Due to recent hacking attempts, security measures have become stricter and require you to store data in encrypted queues.",
            "Which of the following steps can you take to meet your requirements without making changes to the existing code?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Enable SQS KMS encryption",
                "check": true
            },
            {
                "answer": "Use the SSL endpoint",
                "check": false
            },
            {
                "answer": "Use Secrets Manager",
                "check": false
            },
            {
                "answer": "Use Client side encryption",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Enable SQS KMS encryption</strong>Server-side encryption (SSE) lets you transmit sensitive data in encrypted queues. SSE protects the contents of messages in queues using keys managed in AWS Key Management Service\n(AWS KMS).AWS KMS combines secure, highly available hardware and software to provide a key management system scaled for the cloud. When you use Amazon SQS with AWS KMS, the data keys that\nencrypt your message data are also encrypted and stored with the data they protect.You can choose to have SQS encrypt messages stored in both Standard and FIFO queues using an encryption key provided by AWS Key Management Service (KMS).",
            "incorrect": "<strong>Use the SSL endpoint</strong> - The given use-case needs encryption at rest. When using SSL, the data is encrypted during transit, but the data needs to be encrypted at rest\nas well, so this option is incorrect.<strong>Use Client-side encryption</strong> - For additional security, you can build your application to encrypt messages before they are placed in a message queue but will require a\ncode change, so this option is incorrect.<em>*Use Secrets Manager *</em> - AWS Secrets Manager enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle.\nUsers and applications retrieve secrets with a call to Secrets Manager APIs, eliminating the\nneed to hardcode sensitive information in plain text. Secrets Manager offers secret rotation\nwith built-in integration for Amazon RDS, Amazon Redshift, and Amazon DocumentDB. Secrets\nManager cannot be used for encrypting data at rest.Reference:",
            "reference": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-server-side-encryption.html"
        }
    },
    {
        "id": 5,
        "questions": [
            "A company developed an app-based service for citizens to book transportation rides in the local community. The platform is running on AWS EC2 instances and uses Amazon Relational Database Service (RDS) for storing transportation data. A new feature has been requested where receipts would be emailed to customers with PDF attachments retrieved from Amazon Simple Storage Service (S3).",
            "Which of the following options will provide EC2 instances with the right permissions to upload files to Amazon S3 and generate S3 Signed URL?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "EC2 User Data",
                "check": false
            },
            {
                "answer": "CloudFormation",
                "check": false
            },
            {
                "answer": "Run aws configure on the EC2 instance",
                "check": false
            },
            {
                "answer": "Create an IAM Role for EC2",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Create an IAM Role for EC2</strong>IAM roles have been incorporated so that your applications can securely make API requests from your instances, without requiring you to manage the security credentials that the\napplications use. Instead of creating and distributing your AWS credentials, you can\ndelegate permission to make API requests using IAM roles.Amazon EC2 uses an instance profile as a container for an IAM role. When you create an IAM role using the IAM console, the console creates an instance profile automatically and gives\nit the same name as the role to which it corresponds.",
            "incorrect": "<strong>EC2 User Data</strong> - You can specify user data when you launch an instance and you would not want to hard code the AWS credentials in the user data.<strong>Run <code>aws configure</code> on the EC2 instance</strong> - When you first configure the CLI you have to run this command, afterward you should not need to if you want\nto obtain credentials to authenticate to other AWS services. An IAM role will receive\ntemporary credentials for you so you can focus on using the CLI to get access to other AWS\nservices if you have the permissions.<strong>CloudFormation</strong> - AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an\norderly and predictable fashion.Reference:",
            "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/iam-roles-for-amazon-ec2.html"
        }
    },
    {
        "id": 6,
        "questions": [
            "A website serves static content from an Amazon Simple Storage Service (Amazon S3) bucket and dynamic content from an application load balancer. The user base is spread across the world and latency should be minimized for a better user experience.",
            "Which technology/service can help access the static and dynamic content while keeping the data latency low?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Configure CloudFront with multiple origins to serve both static and dynamic content at low latency to global users",
                "check": true
            },
            {
                "answer": "Use CloudFront's Lambda@Edge feature to server data from S3 buckets and load balancer programmatically on-the-fly",
                "check": false
            },
            {
                "answer": "Use Global Accelerator to transparently switch between S3 bucket and load balancer for different data needs",
                "check": false
            },
            {
                "answer": "Use CloudFront's Origin Groups to group both static and dynamic requests into one request for further processing",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Configure CloudFront with multiple origins to serve both static and dynamic content at low latency to global users</strong>Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your\ncontent through a worldwide network of data centers called edge locations. When a user\nrequests content that you're serving with CloudFront, the request is routed to the edge\nlocation that provides the lowest latency (time delay), so that content is delivered with\nthe best possible performance.You can configure a single CloudFront web distribution to serve different types of requests from multiple origins.",
            "incorrect": "<strong>Use CloudFront's Lambda@Edge feature to server data from S3 buckets and load balancer programmatically on-the-fly</strong> - AWS Lambda@Edge is a general-purpose serverless\ncompute feature that supports a wide range of computing needs and customizations.\nLambda@Edge is best suited for computationally intensive operations. This is not relevant\nfor the given use case.<strong>Use Global Accelerator to transparently switch between S3 bucket and load balancer for different data needs</strong> - AWS Global Accelerator is a networking service that\nimproves the performance of your users’ traffic by up to 60% using Amazon Web Services’\nglobal network infrastructure.With Global Accelerator, you are provided two global static public IPs that act as a fixed entry point to your application, improving availability. On the back end, add or remove your\nAWS application endpoints, such as Application Load Balancers, Network Load Balancers, EC2\nInstances, and Elastic IPs without making user-facing changes. Global Accelerator\nautomatically re-routes your traffic to your nearest healthy available endpoint to mitigate\nendpoint failure.CloudFront improves performance for both cacheable content (such as images and videos) and dynamic content (such as API acceleration and dynamic site delivery). Global Accelerator is\na good fit for non-HTTP use cases, such as gaming (UDP), IoT (MQTT), or Voice over IP, as\nwell as for HTTP use cases that specifically require static IP addresses or deterministic,\nfast regional failover.Global Accelerator is not relevant for the given use-case.<strong>Use CloudFront's Origin Groups to group both static and dynamic requests into one request for further processing</strong> - You can set up CloudFront with origin failover\nfor scenarios that require high availability. To get started, you create an Origin Group\nwith two origins: a primary and a secondary. If the primary origin is unavailable or returns\nspecific HTTP response status codes that indicate a failure, CloudFront automatically\nswitches to the secondary origin. Origin Groups are for origin failure scenarios and not for\nrequest routing.References:",
            "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_origin_failover.html"
        }
    },
    {
        "id": 7,
        "questions": [
            "You are designing a high-performance application that requires millions of connections. You have several EC2 instances running Apache2 web servers and the application will require capturing the user’s source IP address and source port without the use of X-Forwarded-For.",
            "Which of the following options will meet your needs?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Network Load Balancer",
                "check": true
            },
            {
                "answer": "Classic Load Balancer",
                "check": false
            },
            {
                "answer": "Application Load Balancer",
                "check": false
            },
            {
                "answer": "Elastic Load Balancer",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Network Load Balancer</strong>A Network Load Balancer functions at the fourth layer of the Open Systems Interconnection (OSI) model. It can handle millions of requests per second. After the load balancer receives\na connection request, it selects a target from the target group for the default rule. It\nattempts to open a TCP connection to the selected target on the port specified in the\nlistener configuration. Incoming connections remain unmodified, so application software need\nnot support X-Forwarded-For.",
            "incorrect": "<strong>Application Load Balancer</strong> - An Application Load Balancer functions at the application layer, the seventh layer of the Open Systems Interconnection (OSI) model. After\nthe load balancer receives a request, it evaluates the listener rules in priority order to\ndetermine which rule to apply and then selects a target from the target group for the rule\naction.One of many benefits of the Application Load Balancer is its support for path-based routing. You can configure rules for your listener that forward requests based on the URL in the\nrequest. This enables you to structure your application as smaller services, and route\nrequests to the correct service based on the content of the URL. For needs relating to\nnetwork traffic go with Network Load Balancer.<strong>Elastic Load Balancer</strong> - Elastic Load Balancing is the service itself that offers different types of load balancers.<strong>Classic Load Balancer</strong> - It is a basic load balancer that distributes traffic. If your account was created before 2013-12-04, your account supports EC2-Classic\ninstances and you will benefit in using this type of load balancer. The classic load\nbalancer can be used regardless of when your account was created and whether you use\nEC2-Classic or whether your instances are in a VPC but just remember its the basic load\nbalancer AWS offers and not advanced as the others.Reference:",
            "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/network/introduction.html"
        }
    },
    {
        "id": 8,
        "questions": [
            "A developer from your team has configured the load balancer to route traffic equally between instances or across Availability Zones. However, Elastic Load Balancing (ELB) routes more traffic to one instance or Availability Zone than the others.",
            "Why is this happening and how can it be fixed? (Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "There could be short-lived TCP connections between clients and instances",
                "check": false
            },
            {
                "answer": "Sticky sessions are enabled for the load balancer",
                "check": true
            },
            {
                "answer": "For Application Load Balancers, cross-zone load balancing is disabled by default",
                "check": false
            },
            {
                "answer": "After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic",
                "check": false
            },
            {
                "answer": "Instances of a specific capacity type aren’t equally distributed across Availability Zones",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Sticky sessions are enabled for the load balancer</strong> - This can be the reason for potential unequal traffic routing by the load balancer. Sticky sessions are a mechanism\nto route requests to the same target in a target group. This is useful for servers that\nmaintain state information in order to provide a continuous experience to clients. To use\nsticky sessions, the clients must support cookies.When a load balancer first receives a request from a client, it routes the request to a target, generates a cookie named AWSALB that encodes information about the selected target,\nencrypts the cookie, and includes the cookie in the response to the client. The client\nshould include the cookie that it receives in subsequent requests to the load balancer. When\nthe load balancer receives a request from a client that contains the cookie, if sticky\nsessions are enabled for the target group and the request goes to the same target group, the\nload balancer detects the cookie and routes the request to the same target.If you use duration-based session stickiness, configure an appropriate cookie expiration time for your specific use case. If you set session stickiness from individual applications, use\nsession cookies instead of persistent cookies where possible.<strong>Instances of a specific capacity type aren’t equally distributed across Availability Zones</strong> - A Classic Load Balancer with HTTP or HTTPS listeners might route more\ntraffic to higher-capacity instance types. This distribution aims to prevent lower-capacity\ninstance types from having too many outstanding requests. It’s a best practice to use\nsimilar instance types and configurations to reduce the likelihood of capacity gaps and\ntraffic imbalances.A traffic imbalance might also occur if you have instances of similar capacities running on different Amazon Machine Images (AMIs). In this scenario, the imbalance of the traffic in\nfavor of higher-capacity instance types is desirable.",
            "incorrect": "<strong>There could be short-lived TCP connections between clients and instances</strong> - This is an incorrect statement. Long-lived TCP connections between clients and instances can\npotentially lead to unequal distribution of traffic by the load balancer. Long-lived TCP\nconnections between clients and instances cause uneven traffic load distribution by design.\nAs a result, new instances take longer to reach connection equilibrium. Be sure to check\nyour metrics for long-lived TCP connections that might be causing routing issues in the load\nbalancer.<strong>For Application Load Balancers, cross-zone load balancing is disabled by default</strong> - This is an incorrect statement. With Application Load Balancers,\ncross-zone load balancing is always enabled.<strong>After you disable an Availability Zone, the targets in that Availability Zone remain registered with the load balancer, thereby receiving random bursts of traffic</strong> -\nThis is an incorrect statement. After you disable an Availability Zone, the targets in that\nAvailability Zone remain registered with the load balancer. However, even though they remain\nregistered, the load balancer does not route traffic to them.References:",
            "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/how-elastic-load-balancing-works.html#availability-zones"
        }
    },
    {
        "id": 9,
        "questions": [
            "You were assigned to a project that requires the use of the AWS CLI to build a project with AWS CodeBuild. Your project's root directory includes the buildspec.yml file to run build commands and would like your build artifacts to be automatically encrypted at the end.",
            "How should you configure CodeBuild to accomplish this?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use In Flight encryption (SSL)",
                "check": false
            },
            {
                "answer": "Use an AWS Lambda Hook",
                "check": false
            },
            {
                "answer": "Use the AWS Encryption SDK",
                "check": false
            },
            {
                "answer": "Specify a KMS key to use",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Specify a KMS key to use</strong>AWS Key Management Service (KMS) makes it easy for you to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications.For AWS CodeBuild to encrypt its build output artifacts, it needs access to an AWS KMS customer master key (CMK). By default, AWS CodeBuild uses the AWS-managed CMK for Amazon S3\nin your AWS account. The following environment variable provides these details:CODEBUILD_KMS_KEY_ID: The identifier of the AWS KMS key that CodeBuild is using to encrypt the build output artifact (for example, arn:aws:kms:region-ID:account-ID:key/key-ID or\nalias/key-alias).",
            "incorrect": "<strong>Use an AWS Lambda Hook</strong> - Code hook is used for integration with Lambda and is not relevant for the given use-case.<strong>Use the AWS Encryption SDK</strong> - The SDK just makes it easier for you to implement encryption best practices in your application and is not relevant for the given\nuse-case.<strong>Use In-Flight encryption (SSL)</strong> - SSL is usually for internet traffic which in this case will be using internal traffic through AWS and is not relevant for the given\nuse-case.References:",
            "reference": "https://docs.aws.amazon.com/codebuild/latest/userguide/setting-up.html"
        }
    },
    {
        "id": 10,
        "questions": [
            "A development team has deployed a REST API in Amazon API Gateway to two different stages - a test stage and a prod stage. The test stage is used as a test build and the prod stage as a stable build. After the updates have passed the test, the team wishes to promote the test stage to the prod stage.",
            "Which of the following represents the optimal solution for this use-case?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Update stage variable value from the stage name of test to that of prod",
                "check": true
            },
            {
                "answer": "Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage",
                "check": false
            },
            {
                "answer": "API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the API to the\nprod stage",
                "check": false
            },
            {
                "answer": "Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Update stage variable value from the stage name of test to that of prod</strong>After creating your API, you must deploy it to make it callable by your users. To deploy an API, you create an API deployment and associate it with a stage. A stage is a logical\nreference to a lifecycle state of your API (for example, dev, prod, beta, v2). API stages\nare identified by the API ID and stage name. They're included in the URL that you use to\ninvoke the API. Each stage is a named reference to a deployment of the API and is made\navailable for client applications to call.Stages enable robust version control of your API. In our current use-case, after the updates pass the test, you can promote the test stage to the prod stage. The promotion can be done\nby redeploying the API to the prod stage or updating a stage variable value from the stage\nname of test to that of prod.",
            "incorrect": "<strong>Deploy the API without choosing a stage. This way, the working deployment will be updated in all stages</strong> - An API can only be deployed to a stage. Hence, it is\nnot possible to deploy an API without choosing a stage.<em>Delete the existing prod stage. Create a new stage with the same name (prod) and deploy the tested version on this stage</em>* - This is possible, but not an optimal way of\ndeploying a change. Also, as prod refers to real production system, this option will result\nin downtime.<strong>API performance is optimized in a different way for prod environments. Hence, promoting test to prod is not correct. The promotion should be done by redeploying the\nAPI to the prod stage</strong> - For each stage, you can optimize API performance by\nadjusting the default account-level request throttling limits and enabling API caching. And\nthese settings can be changed/updated at any time.Reference:",
            "reference": "https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-deploy-api.html"
        }
    },
    {
        "id": 11,
        "questions": [
            "A .NET developer team works with many ASP.NET web applications that use EC2 instances to host them on IIS. The deployment process needs to be configured so that multiple versions of the application can run in AWS Elastic Beanstalk. One version would be used for development, testing, and another version for load testing.",
            "Which of the following methods do you recommend?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "You cannot have multiple development environments in Elastic Beanstalk, just one development and one production environment",
                "check": false
            },
            {
                "answer": "Use only one Beanstalk environment and perform configuration changes using an Ansible script",
                "check": false
            },
            {
                "answer": "Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment",
                "check": true
            },
            {
                "answer": "Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in\n.ebextensions/ to know how to handle the traffic coming from the ALB",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Define a dev environment with a single instance and a 'load test' environment that has settings close to production environment</strong>AWS Elastic Beanstalk makes it easy to create new environments for your application. You can create and manage separate environments for development, testing, and production use, and\nyou can deploy any version of your application to any environment. Environments can be\nlong-running or temporary. When you terminate an environment, you can save its configuration\nto recreate it later.It is common practice to have many environments for the same application. You can deploy multiple environments when you need to run multiple versions of an application. So for the\ngiven use-case, you can set up 'dev' and 'load test' environment.<strong>You cannot have multiple development environments in Elastic Beanstalk, just one development, and one production environment</strong> - Incorrect, use the Create New\nEnvironment wizard in the AWS Management Console for BeanStalk to guide you on this.<strong>Use only one Beanstalk environment and perform configuration changes using an Ansible script</strong> - Ansible is an open-source deployment tool that integrates with AWS. It\nallows us to deploy the infrastructure. Elastic Beanstalk provisions the servers that you\nneed for hosting the application and it also handles multiple environments, so Beanstalk is\na better option.<strong>Create an Application Load Balancer to route based on hostname so you can pass on parameters to the development Elastic Beanstalk environment. Create a file in\n.ebextensions/ to know how to handle the traffic coming from the ALB</strong> - This is\nnot a good design if you need to load test because you will have two versions on the same\ninstances and may not be able to access resources in the system due to the load testing.Reference:",
            "incorrect": "",
            "reference": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.environments.html"
        }
    },
    {
        "id": 12,
        "questions": [
            "You are planning to build a fleet of EBS-optimized EC2 instances to handle the load of your new application. Due to security compliance, your organization wants any secret strings used in the application to be encrypted to prevent exposing values as clear text.",
            "The solution requires that decryption events be audited and API calls to be simple. How can this be achieved? (select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "Store the secret as SecureString in SSM Parameter Store",
                "check": true
            },
            {
                "answer": "Audit using CloudTrail",
                "check": true
            },
            {
                "answer": "Store the secret as PlainText in SSM Parameter Store",
                "check": false
            },
            {
                "answer": "Encrypt first with KMS then store in SSM Parameter store",
                "check": false
            },
            {
                "answer": "Audit using SSM Audit Trail",
                "check": false
            }
        ],
        "explanation": {
            "correct": "",
            "incorrect": "<strong>Encrypt first with KMS then store in SSM Parameter store</strong> - This could work but will require two API calls to get the decrypted value instead of one. So this is not the\nright option.<strong>Store the secret as PlainText in SSM Parameter Store</strong> - Plaintext parameters are not secure and shouldn't be used to store secrets.<strong>Audit using SSM Audit Trail</strong> - This is a made-up option and has been added as a distractor.Reference:",
            "reference": "https://docs.aws.amazon.com/kms/latest/developerguide/services-parameter-store.html"
        }
    },
    {
        "id": 13,
        "questions": [
            "A junior developer working on ECS instances terminated a container instance in Amazon Elastic Container Service (Amazon ECS) as per instructions from the team lead. But the container instance continues to appear as a resource in the ECS cluster.",
            "As a Developer Associate, which of the following solutions would you recommend to fix this behavior?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again",
                "check": false
            },
            {
                "answer": "You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues",
                "check": false
            },
            {
                "answer": "The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues",
                "check": false
            },
            {
                "answer": "You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>You terminated the container instance while it was in STOPPED state, that lead to this synchronization issues</strong> - If you terminate a container instance while it is\nin the STOPPED state, that container instance isn't automatically removed from the cluster.\nYou will need to deregister your container instance in the STOPPED state by using the Amazon\nECS console or AWS Command Line Interface. Once deregistered, the container instance will no\nlonger appear as a resource in your Amazon ECS cluster.",
            "incorrect": "<strong>You terminated the container instance while it was in RUNNING state, that lead to this synchronization issues</strong> - This is an incorrect statement. If you terminate\na container instance in the RUNNING state, that container instance is automatically removed,\nor deregistered, from the cluster.<strong>The container instance has been terminated with AWS CLI, whereas, for ECS instances, Amazon ECS CLI should be used to avoid any synchronization issues</strong> - This is\nincorrect and has been added as a distractor.<strong>A custom software on the container instance could have failed and resulted in the container hanging in an unhealthy state till restarted again</strong> - This is an\nincorrect statement. It is already mentioned in the question that the developer has\nterminated the instance.References:",
            "reference": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ECS_instances.html"
        }
    },
    {
        "id": 14,
        "questions": [
            "A company ingests real-time data into its on-premises data center and subsequently a daily data feed is compressed into a single file and uploaded on Amazon S3 for backup. The typical compressed file size is around 2 GB.",
            "Which of the following is the fastest way to upload the daily compressed file into S3?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Upload the compressed file using multipart upload with S3 transfer acceleration",
                "check": true
            },
            {
                "answer": "Upload the compressed file in a single operation",
                "check": false
            },
            {
                "answer": "Upload the compressed file using multipart upload",
                "check": false
            },
            {
                "answer": "FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Upload the compressed file using multipart upload with S3 transfer acceleration</strong>Amazon S3 Transfer Acceleration enables fast, easy, and secure transfers of files over long distances between your client and an S3 bucket. Transfer Acceleration takes advantage of\nAmazon CloudFront’s globally distributed edge locations. As the data arrives at an edge\nlocation, data is routed to Amazon S3 over an optimized network path.Multipart upload allows you to upload a single object as a set of parts. Each part is a contiguous portion of the object's data. You can upload these object parts independently and\nin any order. If transmission of any part fails, you can retransmit that part without\naffecting other parts. After all parts of your object are uploaded, Amazon S3 assembles\nthese parts and creates the object. If you're uploading large objects over a stable\nhigh-bandwidth network, use multipart uploading to maximize the use of your available\nbandwidth by uploading object parts in parallel for multi-threaded performance. If you're\nuploading over a spotty network, use multipart uploading to increase resiliency to network\nerrors by avoiding upload restarts.",
            "incorrect": "<strong>Upload the compressed file in a single operation</strong> - In general, when your object size reaches 100 MB, you should consider using multipart uploads instead of uploading\nthe object in a single operation. Multipart upload provides improved throughput - you can\nupload parts in parallel to improve throughput. Therefore, this option is not correct.<strong>Upload the compressed file using multipart upload</strong> - Although using multipart upload would certainly speed up the process, combining with S3 transfer acceleration would\nfurther improve the transfer speed. Therefore just using multipart upload is not the correct\noption.<strong>FTP the compressed file into an EC2 instance that runs in the same region as the S3 bucket. Then transfer the file from the EC2 instance into the S3 bucket</strong> - This\nis a roundabout process of getting the file into S3 and added as a distractor. Although it\nis technically feasible to follow this process, it would involve a lot of scripting and\ncertainly would not be the fastest way to get the file into S3.References:",
            "reference": "https://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html"
        }
    },
    {
        "id": 15,
        "questions": [
            "A development team has configured inbound traffic for the relevant ports in both the Security Group of the EC2 instance as well as the Network Access Control List (NACL) of the subnet for the EC2 instance. The team is, however, unable to connect to the service running on the Amazon EC2 instance.",
            "As a developer associate, which of the following will you recommend to fix this issue?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and outbound\ntraffic",
                "check": false
            },
            {
                "answer": "Rules associated with Network ACLs should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic\nbehavior",
                "check": false
            },
            {
                "answer": "Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound\ntraffic",
                "check": true
            },
            {
                "answer": "IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Security Groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and\noutbound traffic</strong> - Security groups are stateful, so allowing inbound traffic to\nthe necessary ports enables the connection. Network ACLs are stateless, so you must allow\nboth inbound and outbound traffic.To enable the connection to a service running on an instance, the associated network ACL must allow both inbound traffic on the port that the service is listening on as well as allow\noutbound traffic from ephemeral ports. When a client connects to a server, a random port\nfrom the ephemeral port range (1024-65535) becomes the client's source port.The designated ephemeral port then becomes the destination port for return traffic from the service, so outbound traffic from the ephemeral port must be allowed in the network ACL.By default, network ACLs allow all inbound and outbound traffic. If your network ACL is more restrictive, then you need to explicitly allow traffic from the ephemeral port range.If you accept traffic from the internet, then you also must establish a route through an internet gateway. If you accept traffic over VPN or AWS Direct Connect, then you must\nestablish a route through a virtual private gateway.",
            "incorrect": "<strong>Network ACLs are stateful, so allowing inbound traffic to the necessary ports enables the connection. Security Groups are stateless, so you must allow both inbound and\noutbound traffic</strong> - This is incorrect as already discussed.<strong>IAM Role defined in the Security Group is different from the IAM Role that is given access in the Network ACLs</strong> - This is a made-up option and just added as a\ndistractor.<strong>Rules associated with Network ACLs should never be modified from the command line. An attempt to modify rules from the command line blocks the rule and results in an erratic\nbehavior</strong> - This option is a distractor. AWS does not support modifying rules of\nNetwork ACLs from the command line tool.Reference:",
            "reference": "https://aws.amazon.com/premiumsupport/knowledge-center/resolve-connection-sg-acl-inbound/"
        }
    },
    {
        "id": 16,
        "questions": [
            "You are working for a shipping company that is automating the creation of ECS clusters with an Auto Scaling Group using an AWS CloudFormation template that accepts cluster name as its parameters. Initially, you launch the template with input value 'MainCluster', which deployed five instances across two availability zones. The second time, you launch the template with an input value 'SecondCluster'. However, the instances created in the second run were also launched in 'MainCluster' even after specifying a different cluster name.",
            "What is the root cause of this issue?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "The ECS agent Docker image must be re-built to connect to the other clusters",
                "check": false
            },
            {
                "answer": "The EC2 instance is missing IAM permissions to join the other clusters",
                "check": false
            },
            {
                "answer": "The security groups on the EC2 instance are pointing to the wrong ECS cluster",
                "check": false
            },
            {
                "answer": "The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>The cluster name Parameter has not been updated in the file /etc/ecs/ecs.config during bootstrap</strong> - In the ecs.config file you have to configure the parameter\nECS_CLUSTER='your_cluster_name' to register the container instance with a cluster named\n'your_cluster_name'.",
            "incorrect": "<strong>The EC2 instance is missing IAM permissions to join the other clusters</strong> - EC2 instances are getting registered to the first cluster, so permissions are not an issue here\nand hence this statement is an incorrect choice for the current use case.<strong>The ECS agent Docker image must be re-built to connect to the other clusters</strong> - Since the first set of instances got created from the template without any issues, there\nis no issue with the ECS agent here.<strong>The security groups on the EC2 instance are pointing to the wrong ECS cluster</strong> - Security groups govern the rules about the incoming network traffic\nto your ECS containers. The issue here is not about user access and hence is a wrong choice\nfor the current use case.References:",
            "reference": "https://docs.aws.amazon.com/AmazonECS/latest/developerguide/launch_container_instance.html"
        }
    },
    {
        "id": 17,
        "questions": [
            "A company follows collaborative development practices. The engineering manager wants to isolate the development effort by setting up simulations of API components owned by various development teams.",
            "Which API integration type is best suited for this requirement?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "MOCK",
                "check": true
            },
            {
                "answer": "AWS_PROXY",
                "check": false
            },
            {
                "answer": "HTTP",
                "check": false
            },
            {
                "answer": "HTTP_PROXY",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>MOCK</strong>This type of integration lets API Gateway return a response without sending the request further to the backend. This is useful for API testing because it can be used to test the\nintegration setup without incurring charges for using the backend and to enable\ncollaborative development of an API.In collaborative development, a team can isolate their development effort by setting up simulations of API components owned by other teams by using the MOCK integrations. It is\nalso used to return CORS-related headers to ensure that the API method permits CORS access.\nIn fact, the API Gateway console integrates the OPTIONS method to support CORS with a mock\nintegration.",
            "incorrect": "<strong>AWS_PROXY</strong> - This type of integration lets an API method be integrated with the Lambda function invocation action with a flexible, versatile, and streamlined\nintegration setup. This integration relies on direct interactions between the client and the\nintegrated Lambda function.<strong>HTTP_PROXY</strong> - The HTTP proxy integration allows a client to access the backend HTTP endpoints with a streamlined integration setup on single API method. You do not\nset the integration request or the integration response. API Gateway passes the incoming\nrequest from the client to the HTTP endpoint and passes the outgoing response from the HTTP\nendpoint to the client.<strong>HTTP</strong> - This type of integration lets an API expose HTTP endpoints in the backend. With the HTTP integration, you must configure both the integration request and\nintegration response. You must set up necessary data mappings from the method request to the\nintegration request, and from the integration response to the method response.Reference:",
            "reference": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-api-integration-types.html"
        }
    },
    {
        "id": 18,
        "questions": [
            "A company's e-commerce application becomes slow when traffic spikes. The application has a three-tier architecture (web, application and database tier) that uses synchronous transactions. The development team at the company has identified certain bottlenecks in the application tier and it is looking for a long term solution to improve the application's performance.",
            "As a developer associate, which of the following solutions would you suggest to meet the required application response times while accounting for any traffic spikes?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS",
                "check": false
            },
            {
                "answer": "Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers",
                "check": false
            },
            {
                "answer": "Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size",
                "check": false
            },
            {
                "answer": "Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Leverage horizontal scaling for the web and application tiers by using Auto Scaling groups and Application Load Balancer</strong> - A horizontally scalable system is one\nthat can increase capacity by adding more computers to the system. This is in contrast to a\nvertically scalable system, which is constrained to running its processes on only one\ncomputer; in such systems, the only way to increase performance is to add more resources\ninto one computer in the form of faster (or more) CPUs, memory or storage.Horizontally scalable systems are oftentimes able to outperform vertically scalable systems by enabling parallel execution of workloads and distributing those across many different\ncomputers.Elastic Load Balancing is used to automatically distribute your incoming application traffic across all the EC2 instances that you are running. You can use Elastic Load Balancing to\nmanage incoming requests by optimally routing traffic so that no one instance is\noverwhelmed.To use Elastic Load Balancing with your Auto Scaling group, you attach the load balancer to your Auto Scaling group to register the group with the load balancer. Your load balancer\nacts as a single point of contact for all incoming web traffic to your Auto Scaling group.When you use Elastic Load Balancing with your Auto Scaling group, it's not necessary to register individual EC2 instances with the load balancer. Instances that are launched by\nyour Auto Scaling group are automatically registered with the load balancer. Likewise,\ninstances that are terminated by your Auto Scaling group are automatically deregistered from\nthe load balancer.This option will require fewer design changes, it's mostly configuration changes and the ability for the web/application tier to be able to communicate across instances. Hence, this\nis the right solution for the current use case.",
            "incorrect": "<strong>Leverage SQS with asynchronous AWS Lambda calls to decouple the application and data tiers</strong> - This is incorrect as it uses asynchronous AWS Lambda calls and the\napplication uses synchronous transactions. The question says there should be no change in\nthe application architecture.<strong>Leverage horizontal scaling for the application's persistence layer by adding Oracle RAC on AWS</strong> - The issue is not with the persistence layer at all. This option\nhas only been used as a distractor.You can deploy scalable Oracle Real Application Clusters (RAC) on Amazon EC2 using Amazon Machine Images (AMI) on AWS Marketplace. Oracle RAC is a shared-everything database cluster\ntechnology from Oracle that allows a single database (a set of data files) to be\nconcurrently accessed and served by one or many database server instances.<strong>Leverage vertical scaling for the application instance by provisioning a larger Amazon EC2 instance size</strong> - Vertical scaling is just a band-aid solution and\nwill not work long term.References:",
            "reference": "https://aws.amazon.com/blogs/compute/operating-lambda-understanding-event-driven-architecture-part-1/"
        }
    },
    {
        "id": 19,
        "questions": [
            "As an AWS certified developer associate, you are working on an AWS CloudFormation template that will create resources for a company's cloud infrastructure. Your template is composed of three stacks which are Stack-A, Stack-B, and Stack-C. Stack-A will provision a VPC, a security group, and subnets for public web applications that will be referenced in Stack-B and Stack-C.",
            "After running the stacks you decide to delete them, in which order should you do it?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Stack B, then Stack C, then Stack A",
                "check": true
            },
            {
                "answer": "Stack A, then Stack B, then Stack C",
                "check": false
            },
            {
                "answer": "Stack A, Stack C then Stack B",
                "check": false
            },
            {
                "answer": "Stack C then Stack A then Stack B",
                "check": false
            }
        ],
        "explanation": {
            "correct": "AWS CloudFormation gives developers and businesses an easy way to create a collection of related AWS and third-party resources and provision them in an orderly and predictable\nfashion.<strong>Stack B, then Stack C, then Stack A</strong>All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you must delete Stack B as well as Stack C, before you delete\nStack A.",
            "incorrect": "<strong>Stack A, then Stack B, then Stack C</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you\ncannot delete Stack A first because that's being referenced in the other Stacks.<strong>Stack A, Stack C then Stack B</strong> - All of the imports must be removed before you can delete the exporting stack or modify the output value. In this case, you cannot\ndelete Stack A first because that's being referenced in the other Stacks.<strong>Stack C then Stack A then Stack B</strong> - Stack C is fine but you should delete Stack B before Stack A because all of the imports must be removed before you can delete the\nexporting stack or modify the output value.",
            "reference": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-stack-exports.html"
        }
    },
    {
        "id": 20,
        "questions": [
            "A security company is requiring all developers to perform server-side encryption with customer-provided encryption keys when performing operations in AWS S3. Developers should write software with C# using the AWS SDK and implement the requirement in the PUT, GET, Head, and Copy operations.",
            "Which of the following encryption methods meets this requirement?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "SSE-S3",
                "check": false
            },
            {
                "answer": "SSE-KMS",
                "check": false
            },
            {
                "answer": "SSE-C",
                "check": true
            },
            {
                "answer": "Client-Side Encryption",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>SSE-C</strong>You have the following options for protecting data at rest in Amazon S3:Server-Side Encryption – Request Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.Client-Side Encryption – Encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and related tools.For the given use-case, the company wants to manage the encryption keys via its custom application and let S3 manage the encryption, therefore you must use Server-Side Encryption\nwith Customer-Provided Keys (SSE-C).Using server-side encryption with customer-provided encryption keys (SSE-C) allows you to set your encryption keys. With the encryption key you provide as part of your request, Amazon S3\nmanages both the encryption, as it writes to disks, and decryption, when you access your\nobjects.",
            "incorrect": "<strong>SSE-KMS</strong> - Server-Side Encryption with Customer Master Keys (CMKs) stored in AWS Key Management Service (SSE-KMS) is similar to SSE-S3. SSE-KMS provides you with an\naudit trail that shows when your CMK was used and by whom. Additionally, you can create and\nmanage customer-managed CMKs or use AWS managed CMKs that are unique to you, your service,\nand your Region.<strong>Client-Side Encryption</strong> - You can encrypt the data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption\nkeys, and related tools.<strong>SSE-S3</strong> - When you use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key. As an additional safeguard, it\nencrypts the key itself with a master key that it regularly rotates. So this option is\nincorrect.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonS3/latest/dev/serv-side-encryption.html"
        }
    },
    {
        "id": 21,
        "questions": [
            "You have a Java-based application running on EC2 instances loaded with AWS CodeDeploy agents. You are considering different options for deployment, one is the flexibility that allows for incremental deployment of your new application versions and replaces existing versions in the EC2 instances. The other option is a strategy in which an Auto Scaling group is used to perform a deployment.",
            "Which of the following options will allow you to deploy in this manner? (Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "Pilot Light Deployment",
                "check": false
            },
            {
                "answer": "Cattle Deployment",
                "check": false
            },
            {
                "answer": "Blue/green Deployment",
                "check": true
            },
            {
                "answer": "In-place Deployment",
                "check": true
            },
            {
                "answer": "Warm Standby Deployment",
                "check": false
            }
        ],
        "explanation": {
            "correct": "",
            "incorrect": "<strong>Cattle Deployment</strong> - This is a good option if you have cattle in a farm<strong>Warm Standby Deployment</strong> - This is not a valid CodeDeploy deployment option. The term \"Warm Standby\" is used to describe a Disaster Recovery scenario in which a\nscaled-down version of a fully functional environment is always running in the cloud.<strong>Pilot Light Deployment</strong> - This is not a valid CodeDeploy deployment option. \"Pilot Light\" is a Disaster Recovery approach where you simply replicate part of your IT\nstructure for a limited set of core services so that the AWS cloud environment seamlessly\ntakes over in the event of a disaster.References:",
            "reference": "https://aws.amazon.com/blogs/publicsector/rapidly-recover-mission-critical-systems-in-a-disaster/"
        }
    },
    {
        "id": 22,
        "questions": [
            "A company has built its technology stack on AWS serverless architecture for managing all its business functions. To expedite development for a new business requirement, the company is looking at using pre-built serverless applications.",
            "Which AWS service represents the easiest solution to address this use-case?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "AWS Serverless Application Repository (SAR)",
                "check": true
            },
            {
                "answer": "AWS Marketplace",
                "check": false
            },
            {
                "answer": "AWS Service Catalog",
                "check": false
            },
            {
                "answer": "AWS AppSync",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>AWS Serverless Application Repository (SAR)</strong> - The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams,\norganizations, and individual developers to store and share reusable applications, and\neasily assemble and deploy serverless architectures in powerful new ways. Using the\nServerless Application Repository, you don't need to clone, build, package, or publish\nsource code to AWS before deploying it. Instead, you can use pre-built applications from the\nServerless Application Repository in your serverless architectures, helping you and your\nteams reduce duplicated work, ensure organizational best practices, and get to market\nfaster. Integration with AWS Identity and Access Management (IAM) provides resource-level\ncontrol of each application, enabling you to publicly share applications with everyone or\nprivately share them with specific AWS accounts.",
            "incorrect": "<strong>AWS Marketplace</strong> - The AWS Marketplace enables qualified partners to market and sell their software to AWS Customers. AWS Marketplace is an online software store that\nhelps customers find, buy, and immediately start using the software and services that run on\nAWS. AWS Marketplace is designed for Independent Software Vendors (ISVs), Value-Added\nResellers (VARs), and Systems Integrators (SIs) who have software products they want to\noffer to customers in the cloud.<strong>AWS AppSync</strong> - AWS AppSync is a fully managed service that makes it easy to develop GraphQL APIs by handling the heavy lifting of securely connecting to data sources\nlike AWS DynamoDB, Lambda, and more. Organizations choose to build APIs with GraphQL because\nit helps them develop applications faster, by giving front-end developers the ability to\nquery multiple databases, microservices, and APIs with a single GraphQL endpoint.<strong>AWS Service Catalog</strong> - AWS Service Catalog allows organizations to create and manage catalogs of IT services that are approved for use on AWS. These IT services can\ninclude everything from virtual machine images, servers, software, and databases to complete\nmulti-tier application architectures. AWS Service Catalog allows you to centrally manage\ndeployed IT services and your applications, resources, and metadata. This helps you achieve\nconsistent governance and meet your compliance requirements while enabling users to quickly\ndeploy only the approved IT services they need.Reference:",
            "reference": "https://aws.amazon.com/serverless/serverlessrepo/"
        }
    },
    {
        "id": 23,
        "questions": [
            "An Auto Scaling group has a maximum capacity of 3, a current capacity of 2, and a scaling policy that adds 3 instances.",
            "When executing this scaling policy, what is the expected outcome?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Amazon EC2 Auto Scaling adds only 1 instance to the group",
                "check": true
            },
            {
                "answer": "Amazon EC2 Auto Scaling adds 3 instances to the group",
                "check": false
            },
            {
                "answer": "Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually",
                "check": false
            },
            {
                "answer": "Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance",
                "check": false
            }
        ],
        "explanation": {
            "correct": "A scaling policy instructs Amazon EC2 Auto Scaling to track a specific CloudWatch metric, and it defines what action to take when the associated CloudWatch alarm is in ALARM.When a scaling policy is executed, if the capacity calculation produces a number outside of the minimum and maximum size range of the group, Amazon EC2 Auto Scaling ensures that the\nnew capacity never goes outside of the minimum and maximum size limits.<strong>Amazon EC2 Auto Scaling adds only 1 instance to the group</strong>For the given use-case, Amazon EC2 Auto Scaling adds only 1 instance to the group to prevent the group from exceeding its maximum size.",
            "incorrect": "<strong>Amazon EC2 Auto Scaling adds 3 instances to the group</strong> - This is an incorrect statement. Auto Scaling ensures that the new capacity never goes outside of the minimum and\nmaximum size limits.<strong>Amazon EC2 Auto Scaling adds 3 instances to the group and scales down 2 of those instances eventually</strong> - This is an incorrect statement. Adding the instances\ninitially and immediately downsizing them is impractical.<strong>Amazon EC2 Auto Scaling does not add any instances to the group, but suggests changing the scaling policy to add one instance</strong> - This option has been added as\na distractor.Reference:",
            "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scale-based-on-demand.html"
        }
    },
    {
        "id": 24,
        "questions": [
            "A company wants to automate its order fulfillment and inventory tracking workflow. Starting from order creation to updating inventory to shipment, the entire process has to be tracked, managed and updated automatically.",
            "Which of the following would you recommend as the most optimal solution for this requirement?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow",
                "check": true
            },
            {
                "answer": "Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow",
                "check": false
            },
            {
                "answer": "Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems",
                "check": false
            },
            {
                "answer": "Use Amazon SNS to develop event-driven applications that can share information",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Use AWS Step Functions to coordinate and manage the components of order management and inventory tracking workflow</strong>AWS Step Functions is a serverless function orchestrator that makes it easy to sequence AWS Lambda functions and multiple AWS services into business-critical applications. Through its\nvisual interface, you can create and run a series of checkpointed and event-driven workflows\nthat maintain the application state. The output of one step acts as an input to the next.\nEach step in your application executes in order, as defined by your business logic.AWS Step Functions enables you to implement a business process as a series of steps that make up a workflow. The individual steps in the workflow can invoke a Lambda function or a\ncontainer that has some business logic, update a database such as DynamoDB or publish a\nmessage to a queue once that step or the entire workflow completes execution.Benefits of Step Functions:Build and update apps quickly: AWS Step Functions lets you build visual workflows that enable the fast translation of business requirements into technical requirements. You can build\napplications in a matter of minutes, and when needs change, you can swap or reorganize\ncomponents without customizing any code.Improve resiliency: AWS Step Functions manages state, checkpoints and restarts for you to make sure that your application executes in order and as expected. Built-in try/catch, retry\nand rollback capabilities deal with errors and exceptions automatically.Write less code: AWS Step Functions manages the logic of your application for you and implements basic primitives such as branching, parallel execution, and timeouts. This\nremoves extra code that may be repeated in your microservices and functions.",
            "incorrect": "<strong>Use Amazon Simple Queue Service (Amazon SQS) queue to pass information from order management to inventory tracking workflow</strong> - You should consider AWS Step\nFunctions when you need to coordinate service components in the development of highly\nscalable and auditable applications. You should consider using Amazon Simple Queue Service\n(Amazon SQS), when you need a reliable, highly scalable, hosted queue for sending, storing,\nand receiving messages between services. Step Functions keeps track of all tasks and events\nin an application. Amazon SQS requires you to implement your own application-level tracking,\nespecially if your application uses multiple queues.<strong>Configure Amazon EventBridge to track the flow of work from order management to inventory tracking systems</strong> - Both Amazon EventBridge and Amazon SNS can be used\nto develop event-driven applications, and your choice will depend on your specific needs.\nAmazon EventBridge is recommended when you want to build an application that reacts to\nevents from SaaS applications and/or AWS services. Amazon EventBridge is the only\nevent-based service that integrates directly with third-party SaaS partners.<strong>Use Amazon SNS to develop event-driven applications that can share information</strong> - Amazon SNS is recommended when you want to build an application\nthat reacts to high throughput or low latency messages published by other applications or\nmicroservices (as Amazon SNS provides nearly unlimited throughput), or for applications that\nneed very high fan-out (thousands or millions of endpoints).References:",
            "reference": "https://aws.amazon.com/eventbridge/faqs/"
        }
    },
    {
        "id": 25,
        "questions": [
            "A Developer at a company is working on a CloudFormation template to set up resources. Resources will be defined using code and provisioned based on certain conditions defined in the Conditions section.",
            "Which section of a CloudFormation template cannot be associated with Condition?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Conditions",
                "check": false
            },
            {
                "answer": "Parameters",
                "check": true
            },
            {
                "answer": "Resources",
                "check": false
            },
            {
                "answer": "Outputs",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Parameters</strong>The optional Conditions section contains statements that define the circumstances under which entities are created or configured. For example, you can create a condition and then\nassociate it with a resource or output so that AWS CloudFormation only creates the resource\nor output if the condition is true.You might use conditions when you want to reuse a template that can create resources in different contexts, such as a test environment versus a production environment. In your\ntemplate, you can add an EnvironmentType input parameter, which accepts either prod or test\nas inputs. For the production environment, you might include Amazon EC2 instances with\ncertain capabilities; however, for the test environment, you want to use reduced\ncapabilities to save money.Conditions cannot be used within the Parameters section. After you define all your conditions, you can associate them with resources and resource properties only in the\nResources and Outputs sections of a template.",
            "incorrect": "<strong>Resources</strong> - Resources section describes the resources that you want to provision in your AWS CloudFormation stacks. You can associate conditions with the resources\nthat you want to conditionally create.<strong>Conditions</strong> - You actually define conditions in this section of the CloudFormation template<strong>Outputs</strong> - The optional Outputs section declares output values that you can import into other stacks (to create cross-stack references), return in response (to describe\nstack calls), or view on the AWS CloudFormation console. For example, you can output the S3\nbucket name for a stack to make the bucket easier to find. You can associate conditions with\nthe outputs that you want to conditionally create.References:",
            "reference": "https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html"
        }
    },
    {
        "id": 26,
        "questions": [
            "A developer is testing Amazon Simple Queue Service (SQS) queues in a development environment. The queue along with all its contents has to be deleted after testing.",
            "Which SQS API should be used for this requirement?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "DeleteQueue",
                "check": true
            },
            {
                "answer": "RemoveQueue",
                "check": false
            },
            {
                "answer": "RemovePermission",
                "check": false
            },
            {
                "answer": "PurgeQueue",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>DeleteQueue</strong> - Deletes the queue specified by the QueueUrl, regardless of the queue's contents. When you delete a queue, any messages in the queue are no longer\navailable.When you delete a queue, the deletion process takes up to 60 seconds. Requests you send involving that queue during the 60 seconds might succeed. For example, a SendMessage request\nmight succeed, but after 60 seconds the queue and the message you sent no longer exist.When you delete a queue, you must wait at least 60 seconds before creating a queue with the same name.",
            "incorrect": "<strong>PurgeQueue</strong> - Deletes the messages in a queue specified by the QueueURL parameter. When you use the PurgeQueue action, you can't retrieve any messages deleted from\na queue. The queue however remains.<strong>RemoveQueue</strong> - This is an invalid option, given only as a distractor.<strong>RemovePermission</strong> - Revokes any permissions in the queue policy that matches the specified Label parameter.Reference:",
            "reference": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_RemovePermission.html"
        }
    },
    {
        "id": 27,
        "questions": [
            "The Development team at a media company is working on securing their databases.",
            "Which of the following AWS database engines can be configured with IAM Database Authentication? (Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "RDS PostGreSQL",
                "check": true
            },
            {
                "answer": "RDS MySQL",
                "check": true
            },
            {
                "answer": "RDS Oracle",
                "check": false
            },
            {
                "answer": "RDS SQL Server",
                "check": false
            },
            {
                "answer": "RDS Db2",
                "check": false
            }
        ],
        "explanation": {
            "correct": "",
            "incorrect": "<strong>RDS Oracle</strong><strong>RDS SQL Server</strong>These two options contradict the details in the explanation above, so these are incorrect.<strong>RDS Db2</strong> - This option has been added as a distractor. Db2 is a family of data management products, including database servers, developed by IBM. RDS does not support\nDb2 database engine.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html"
        }
    },
    {
        "id": 28,
        "questions": [
            "When running a Rolling deployment in Elastic Beanstalk environment, only two batches completed the deployment successfully, while rest of the batches failed to deploy the updated version. Following this, the development team terminated the instances from the failed deployment.",
            "What will be the status of these failed instances post termination?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Elastic Beanstalk will not replace the failed instances",
                "check": false
            },
            {
                "answer": "Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment",
                "check": false
            },
            {
                "answer": "Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console",
                "check": false
            },
            {
                "answer": "Elastic Beanstalk will replace the failed instances with instances running the application version from the most recent successful deployment",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Elastic Beanstalk will replace them with instances running the application version from the most recent successful deployment</strong>When processing a batch, Elastic Beanstalk detaches all instances in the batch from the load balancer, deploys the new application version, and then reattaches the instances. If you\nenable connection draining, Elastic Beanstalk drains existing connections from the Amazon\nEC2 instances in each batch before beginning the deployment.If a deployment fails after one or more batches completed successfully, the completed batches run the new version of your application while any pending batches continue to run the old\nversion. You can identify the version running on the instances in your environment on the\nhealth page in the console. This page displays the deployment ID of the most recent\ndeployment that was executed on each instance in your environment. If you terminate\ninstances from the failed deployment, Elastic Beanstalk replaces them with instances running\nthe application version from the most recent successful deployment.",
            "incorrect": "<strong>Elastic Beanstalk will not replace the failed instances</strong><strong>Elastic Beanstalk will replace the failed instances with instances running the application version from the oldest successful deployment</strong><strong>Elastic Beanstalk will replace the failed instances after the application version to be installed is manually chosen from AWS Console</strong>These three options contradict the explanation provided above, so these options are incorrect.Reference:",
            "reference": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features.rolling-version-deploy.html"
        }
    },
    {
        "id": 29,
        "questions": [
            "A company has recently launched a new gaming application that the users are adopting rapidly. The company uses RDS MySQL as the database. The development team wants an urgent solution to this issue where the rapidly increasing workload might exceed the available database storage.",
            "As a developer associate, which of the following solutions would you recommend so that it requires minimum development effort to address this requirement?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Create read replica for RDS MySQL",
                "check": false
            },
            {
                "answer": "Migrate RDS MySQL database to Aurora which offers storage auto-scaling",
                "check": false
            },
            {
                "answer": "Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required",
                "check": false
            },
            {
                "answer": "Enable storage auto-scaling for RDS MySQL",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Enable storage auto-scaling for RDS MySQL</strong>If your workload is unpredictable, you can enable storage autoscaling for an Amazon RDS DB instance. With storage autoscaling enabled, when Amazon RDS detects that you are running out\nof free database space it automatically scales up your storage. Amazon RDS starts a storage\nmodification for an autoscaling-enabled DB instance when these factors apply:Free available space is less than 10 percent of the allocated storage.The low-storage condition lasts at least five minutes.At least six hours have passed since the last storage modification.The maximum storage threshold is the limit that you set for autoscaling the DB instance. You can't set the maximum storage threshold for autoscaling-enabled instances to a value greater\nthan the maximum allocated storage.",
            "incorrect": "<strong>Migrate RDS MySQL to Aurora which offers storage auto-scaling</strong> - Although Aurora offers automatic storage scaling, this option is ruled out since it involves\nsignificant systems administration effort to migrate from RDS MySQL to Aurora. It is much\neasier to just enable storage auto-scaling for RDS MySQL.<strong>Migrate RDS MySQL database to DynamoDB which automatically allocates storage space when required</strong> - This option is ruled out since DynamoDB is a NoSQL database\nwhich implies significant development effort to change the application logic to connect and\nquery data from the underlying database. It is much easier to just enable storage\nauto-scaling for RDS MySQL.<strong>Create read replica for RDS MySQL</strong> - Read replicas make it easy to take advantage of supported engines' built-in replication functionality to elastically scale out\nbeyond the capacity constraints of a single DB instance for read-heavy database workloads.\nYou can create multiple read replicas for a given source DB Instance and distribute your\napplication’s read traffic amongst them. This option acts as a distractor as read replicas\ncannot help to automatically scale storage for the primary database.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_PIOPS.StorageTypes.html"
        }
    },
    {
        "id": 30,
        "questions": [
            "While defining a business workflow as state machine on AWS Step Functions, a developer has configured several states.",
            "Which of the following would you identify as the state that represents a single unit of work performed by a state machine?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "\"HelloWorld\": { \"Type\": \"Task\",\n\"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:HelloFunction\",\n\"Next\": \"AfterHelloWorldState\",\n\"Comment\": \"Run the HelloWorld Lambda function\"\n}",
                "check": true
            },
            {
                "answer": "\"No-op\": { \"Type\": \"Task\",\n\"Result\": {\n\"x-datum\": 0.381018,\n\"y-datum\": 622.2269926397355\n},\n\"ResultPath\": \"$.coords\",\n\"Next\": \"End\"\n}",
                "check": false
            },
            {
                "answer": "\"wait_until\" : { \"Type\": \"Wait\",\n\"Timestamp\": \"2016-03-14T01:59:00Z\",\n\"Next\": \"NextState\"\n}",
                "check": false
            },
            {
                "answer": "\"FailState\": { \"Type\": \"Fail\",\n\"Cause\": \"Invalid response.\",\n\"Error\": \"ErrorA\"\n}",
                "check": false
            }
        ],
        "explanation": {
            "correct": "A Task state (\"Type\": \"Task\") represents a single unit of work performed by a state machine.All work in your state machine is done by tasks. A task performs work by using an activity or an AWS Lambda function, or by passing parameters to the API actions of other services.AWS Step Functions can invoke Lambda functions directly from a task state. A Lambda function is a cloud-native task that runs on AWS Lambda. You can write Lambda functions in a variety\nof programming languages, using the AWS Management Console or by uploading code to Lambda.",
            "incorrect": "Reference:",
            "reference": "https://docs.aws.amazon.com/step-functions/latest/dg/amazon-states-language-task-state.html"
        }
    },
    {
        "id": 31,
        "questions": [
            "A development team has configured an Elastic Load Balancer for host-based routing. The idea is to support multiple subdomains and different top-level domains.",
            "The rule *.sample.com matches which of the following?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "test.sample.com",
                "check": true
            },
            {
                "answer": "sample.com",
                "check": false
            },
            {
                "answer": "SAMPLE.COM",
                "check": false
            },
            {
                "answer": "sample.test.com",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>test.sample.com</strong> - You can use host conditions to define rules that route requests based on the hostname in the host header (also known as host-based routing). This\nenables you to support multiple subdomains and different top-level domains using a single\nload balancer.A hostname is not case-sensitive, can be up to 128 characters in length, and can contain any of the following characters:\n1. A–Z, a–z, 0–9\n2. - .\n3. * (matches 0 or more characters)\n4. ? (matches exactly 1 character)You must include at least one \".\" character. You can include only alphabetical characters after the final \".\" character.The rule *.sample.com matches test.sample.com but doesn't match sample.com.",
            "incorrect": "<strong>sample.com</strong><strong>sample.test.com</strong><strong>SAMPLE.COM</strong>These three options contradict the explanation provided above, so these options are incorrect.Reference:",
            "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html"
        }
    },
    {
        "id": 32,
        "questions": [
            "Your company has been hired to build a resilient mobile voting app for an upcoming music award show that expects to have 5 to 20 million viewers. The mobile voting app will be marketed heavily months in advance so you are expected to handle millions of messages in the system. You are configuring Amazon Simple Queue Service (SQS) queues for your architecture that should receive messages from 20 KB to 200 KB.",
            "Is it possible to send these messages to SQS?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "No, the max message size is 64KB",
                "check": false
            },
            {
                "answer": "Yes, the max message size is 256KB",
                "check": true
            },
            {
                "answer": "Yes, the max message size is 512KB",
                "check": false
            },
            {
                "answer": "No, the max message size is 128KB",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Yes, the max message size is 256KB</strong>The minimum message size is 1 byte (1 character). The maximum is 262,144 bytes (256 KB).",
            "incorrect": "<strong>Yes, the max message size is 512KB</strong> - The max size is 256KB<strong>No, the max message size is 128KB</strong> - The max size is 256KB<strong>No, the max message size is 64KB</strong> - The max size is 256KBReference:",
            "reference": "https://aws.amazon.com/sqs/faqs/"
        }
    },
    {
        "id": 33,
        "questions": [
            "A developer has created a new Application Load Balancer but has not registered any targets with the target groups.",
            "Which of the following errors would be generated by the Load Balancer?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "HTTP 504: Gateway timeout",
                "check": false
            },
            {
                "answer": "HTTP 502: Bad gateway",
                "check": false
            },
            {
                "answer": "HTTP 500: Internal server error",
                "check": false
            },
            {
                "answer": "HTTP 503: Service unavailable",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>HTTP 503: Service unavailable</strong>The Load Balancer generates the <code>HTTP 503: Service unavailable</code> error when the target groups for the load balancer have no registered targets.",
            "incorrect": "<strong>HTTP 500: Internal server error</strong><strong>HTTP 502: Bad gateway</strong><strong>HTTP 504: Gateway timeout</strong>Here is a summary of the possible causes for these error types:Reference:",
            "reference": "https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-troubleshooting.html"
        }
    },
    {
        "id": 34,
        "questions": [
            "Amazon Simple Queue Service (SQS) has a set of APIs for various actions supported by the service.",
            "As a developer associate, which of the following would you identify as correct regarding the CreateQueue API? (Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using MessageRetentionPeriod attribute",
                "check": false
            },
            {
                "answer": "The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue",
                "check": false
            },
            {
                "answer": "Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag",
                "check": false
            },
            {
                "answer": "You can't change the queue type after you create it",
                "check": true
            },
            {
                "answer": "The visibility timeout value for the queue is in seconds, which defaults to 30 seconds",
                "check": true
            }
        ],
        "explanation": {
            "correct": "",
            "incorrect": "<strong>The dead-letter queue of a FIFO queue must also be a FIFO queue. Whereas, the dead-letter queue of a standard queue can be a standard queue or a FIFO queue</strong> -\nThe dead-letter queue of a FIFO queue must also be a FIFO queue. Similarly, the dead-letter\nqueue of a standard queue must also be a standard queue.<strong>The length of time, in seconds, for which the delivery of all messages in the queue is delayed is configured using <code>MessageRetentionPeriod</code> attribute</strong> -\nThe length of time, in seconds, for which the delivery of all messages in the queue is\ndelayed is configured using <code>DelaySeconds</code> attribute.\n<code>MessageRetentionPeriod</code> attribute controls the length of time, in seconds, for\nwhich Amazon SQS retains a message.<strong>Queue tags are case insensitive. A new tag with a key identical to that of an existing tag overwrites the existing tag</strong> - Queue tags are case-sensitive. A new\ntag with a key identical to that of an existing tag overwrites the existing tag. To be able\nto tag a queue on creation, you must have the <code>sqs:CreateQueue</code> and\n<code>sqs:TagQueue</code> permissions.Reference:",
            "reference": "https://docs.aws.amazon.com/AWSSimpleQueueService/latest/APIReference/API_CreateQueue.html"
        }
    },
    {
        "id": 35,
        "questions": [
            "The development team at a health-care company is planning to migrate to AWS Cloud from the on-premises data center. The team is evaluating Amazon RDS as the database tier for its flagship application.",
            "Which of the following would you identify as correct for RDS Multi-AZ? (Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "RDS applies OS updates by performing maintenance on the standby, then promoting the standby to primary and finally performing maintenance on the old primary, which becomes\nthe new standby",
                "check": true
            },
            {
                "answer": "Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync",
                "check": false
            },
            {
                "answer": "Amazon RDS automatically initiates a failover to the standby, in case primary database fails for any reason",
                "check": true
            },
            {
                "answer": "To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests",
                "check": false
            },
            {
                "answer": "For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB",
                "check": false
            }
        ],
        "explanation": {
            "correct": "",
            "incorrect": "<strong>For automated backups, I/O activity is suspended on your primary DB since backups are not taken from standby DB</strong> - The availability benefits of Multi-AZ also extend\nto planned maintenance. For example, with automated backups, I/O activity is no longer\nsuspended on your primary during your preferred backup window, since backups are taken from\nthe standby.<strong>To enhance read scalability, a Multi-AZ standby instance can be used to serve read requests</strong> - A Multi-AZ standby cannot serve read requests. Multi-AZ deployments\nare designed to provide enhanced database availability and durability, rather than read\nscaling benefits. As such, the feature uses synchronous replication between primary and\nstandby. AWS implementation makes sure the primary and the standby are constantly in sync,\nbut precludes using the standby for read or write operations.<strong>Updates to your DB Instance are asynchronously replicated across the Availability Zone to the standby in order to keep both in sync</strong> - When you create your DB\ninstance to run as a Multi-AZ deployment, Amazon RDS automatically provisions and maintains\na synchronous “standby” replica in a different Availability Zone. Updates to your DB\nInstance are synchronously replicated across the Availability Zone to the standby in order\nto keep both in sync and protect your latest database updates against DB instance failure.Reference:",
            "reference": "https://aws.amazon.com/rds/faqs/"
        }
    },
    {
        "id": 36,
        "questions": [
            "A developer is configuring a bucket policy that denies upload object permission to any requests that do not include the x-amz-server-side-encryption header requesting server-side encryption with SSE-KMS for an Amazon S3 bucket - examplebucket.",
            "Which of the following policies is the right fit for the given requirement?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"*\",\n\"Action\":\"s3:PutObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/*\",\n\"Condition\":{\n\"StringNotEquals\":{\n\"s3:x-amz-server-side-encryption\":\"false\"\n}\n}\n}\n]\n}",
                "check": false
            },
            {
                "answer": "{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"*\",\n\"Action\":\"s3:PutObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/*\",\n\"Condition\":{\n\"StringNotEquals\":{\n\"s3:x-amz-server-side-encryption\":\"aws:kms\"\n}\n}\n}\n]\n}",
                "check": true
            },
            {
                "answer": "{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"*\",\n\"Action\":\"s3:PutObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/*\",\n\"Condition\":{\n\"StringEquals\":{\n\"s3:x-amz-server-side-encryption\":\"aws:kms\"\n}\n}\n}\n]\n}",
                "check": false
            },
            {
                "answer": "{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"*\",\n\"Action\":\"s3:GetObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/*\",\n\"Condition\":{\n\"StringNotEquals\":{\n\"s3:x-amz-server-side-encryption\":\"aws:AES256\"\n}\n}\n}\n]\n}",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"<em>\",\n\"Action\":\"s3:PutObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n\"Condition\":{\n\"StringNotEquals\":{\n\"s3:x-amz-server-side-encryption\":\"aws:kms\"\n}\n}\n}\n]\n}</strong> - This bucket policy denies upload object (s3:PutObject) permission if the\nrequest does not include the x-amz-server-side-encryption header requesting server-side\nencryption with SSE-KMS. To ensure that a particular AWS KMS CMK be used to encrypt the\nobjects in a bucket, you can use the\n<code>s3:x-amz-server-side-encryption-aws-kms-key-id</code> condition key. To specify the\nAWS KMS CMK, you must use a key Amazon Resource Name (ARN) that is in the\n\"arn:aws:kms:region:acct-id:key/key-id\" format.When you upload an object, you can specify the AWS KMS CMK using the <code>x-amz-server-side-encryption-aws-kms-key-id</code> header. If the header is not\npresent in the request, Amazon S3 assumes the AWS-managed CMK.",
            "incorrect": "<strong>{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"<em>\",\n\"Action\":\"s3:PutObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n\"Condition\":{\n\"StringEquals\":{\n\"s3:x-amz-server-side-encryption\":\"aws:kms\"\n}\n}\n}\n]\n}</strong> - The condition is incorrect in this policy. The condition should use\nStringNotEquals.<strong>{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"<em>\",\n\"Action\":\"s3:GetObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n\"Condition\":{\n\"StringNotEquals\":{\n\"s3:x-amz-server-side-encryption\":\"aws:AES256\"\n}\n}\n}\n]\n}</strong> - AES256 is used for Amazon S3-managed encryption keys (SSE-S3). Amazon S3\nserver-side encryption uses one of the strongest block ciphers available to encrypt your\ndata, 256-bit Advanced Encryption Standard (AES-256).<strong>{ \"Version\":\"2012-10-17\",\n\"Id\":\"PutObjectPolicy\",\n\"Statement\":[{\n\"Sid\":\"DenyUnEncryptedObjectUploads\",\n\"Effect\":\"Deny\",\n\"Principal\":\"<em>\",\n\"Action\":\"s3:PutObject\",\n\"Resource\":\"arn:aws:s3:::examplebucket/</em>\",\n\"Condition\":{\n\"StringNotEquals\":{\n\"s3:x-amz-server-side-encryption\":\"false\"\n}\n}\n}\n]\n}</strong> - The condition is incorrect in this policy. The condition should use\n<code>\"s3:x-amz-server-side-encryption\":\"aws:kms\"</code>.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingKMSEncryption.html"
        }
    },
    {
        "id": 37,
        "questions": [
            "A Developer is configuring Amazon EC2 Auto Scaling group to scale dynamically.",
            "Which metric below is NOT part of Target Tracking Scaling Policy?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "ALBRequestCountPerTarget",
                "check": false
            },
            {
                "answer": "ASGAverageCPUUtilization",
                "check": false
            },
            {
                "answer": "ASGAverageNetworkOut",
                "check": false
            },
            {
                "answer": "ApproximateNumberOfMessagesVisible",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>ApproximateNumberOfMessagesVisible</strong> - This is a CloudWatch Amazon SQS queue metric. The number of messages in a queue might not change proportionally to the size of the\nAuto Scaling group that processes messages from the queue. Hence, this metric does not work\nfor target tracking.",
            "incorrect": "With target tracking scaling policies, you select a scaling metric and set a target value. Amazon EC2 Auto Scaling creates and manages the CloudWatch alarms that trigger the scaling\npolicy and calculates the scaling adjustment based on the metric and the target value.It is important to note that a target tracking scaling policy assumes that it should scale out your Auto Scaling group when the specified metric is above the target value. You cannot\nuse a target tracking scaling policy to scale out your Auto Scaling group when the specified\nmetric is below the target value.<strong>ASGAverageCPUUtilization</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average CPU utilization of the Auto Scaling group.<strong>ASGAverageNetworkOut</strong> - This is a predefined metric for target tracking scaling policy. This represents the Average number of bytes sent out on all network\ninterfaces by the Auto Scaling group.<strong>ALBRequestCountPerTarget</strong> - This is a predefined metric for target tracking scaling policy. This represents the Number of requests completed per target in an\nApplication Load Balancer target group.Reference:",
            "reference": "https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-target-tracking.html"
        }
    },
    {
        "id": 38,
        "questions": [
            "You are responsible for an application that runs on multiple Amazon EC2 instances. In front of the instances is an Internet-facing load balancer that takes requests from clients over the internet and distributes them to the EC2 instances. A health check is configured to ping the index.html page found in the root directory for the health status. When accessing the website via the internet visitors of the website receive timeout errors.",
            "What should be checked first to resolve the issue?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Security Groups",
                "check": true
            },
            {
                "answer": "The application is down",
                "check": false
            },
            {
                "answer": "The ALB is warming up",
                "check": false
            },
            {
                "answer": "IAM Roles",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Security Groups</strong>A security group acts as a virtual firewall for your EC2 instances to control incoming and outgoing traffic. Inbound rules control the incoming traffic to your instance, and outbound\nrules control the outgoing traffic from your instance.Check the security group rules of your EC2 instance. You need a security group rule that allows inbound traffic from your public IPv4 address on the proper port.",
            "incorrect": "<strong>IAM Roles</strong> - Usually you run into issues with authorization of APIs with roles but not for timeout, so this option does not fit the given use-case.<strong>The application is down</strong> - Although you can set a health check for application ping or HTTP, timeouts are usually caused by blocked firewall access.<strong>The ALB is warming up</strong> - ALB has a slow start mode which allows a warm-up period before being able to respond to requests with optimal performance. So this is not the\nissue.Reference:",
            "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html#TroubleshootingInstancesConnectionTimeout"
        }
    },
    {
        "id": 39,
        "questions": [
            "You team maintains a public API Gateway that is accessed by clients from another domain. Usage has been consistent for the last few months but recently it has more than doubled. As a result, your costs have gone up and would like to prevent other unauthorized domains from accessing your API.",
            "Which of the following actions should you take?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use Mapping Templates",
                "check": false
            },
            {
                "answer": "Restrict access by using CORS",
                "check": true
            },
            {
                "answer": "Use Account-level throttling",
                "check": false
            },
            {
                "answer": "Assign a Security Group to your API Gateway",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Restrict access by using CORS</strong> - Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources\nin a different domain. When your API's resources receive requests from a domain other than\nthe API's own domain and you want to restrict servicing these requests, you must disable\ncross-origin resource sharing (CORS) for selected methods on the resource.",
            "incorrect": "<strong>Use Account-level throttling</strong> - To prevent your API from being overwhelmed by too many requests, Amazon API Gateway throttles requests to your API. By default, API\nGateway limits the steady-state request rate to 10,000 requests per second (rps). It limits\nthe burst (that is, the maximum bucket size) to 5,000 requests across all APIs within an AWS\naccount. This is Account-level throttling. As you see, this is about limit on the number of\nrequests and is not a suitable answer for the current scenario.<strong>Use Mapping Templates</strong> - A mapping template is a script expressed in Velocity Template Language (VTL) and applied to the payload using JSONPath expressions. Mapping\ntemplates help format/structure the data in a way that it is easily readable, unlike a\nserver response that might always be easy to ready. Mapping Templates have nothing to do\nwith access and are not useful for the current scenario.<strong>Assign a Security Group to your API Gateway</strong> - API Gateway does not use security groups but uses resource policies, which are JSON policy documents that you attach\nto an API to control whether a specified principal (typically an IAM user or role) can\ninvoke the API. You can restrict IP address using this, the downside being, an IP address\ncan be changed by the accessing user. So, this is not an optimal solution for the current\nuse case.References:",
            "reference": "https://docs.aws.amazon.com/apigateway/latest/developerguide/rest-api-data-transformations.html"
        }
    },
    {
        "id": 40,
        "questions": [
            "A development team has noticed that one of the EC2 instances has been wrongly configured with the 'DeleteOnTermination' attribute set to True for its root EBS volume.",
            "As a developer associate, can you suggest a way to disable this flag while the instance is still running?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Set the DeleteOnTermination attribute to False using the command line",
                "check": true
            },
            {
                "answer": "Set the DisableApiTermination attribute of the instance using the API",
                "check": false
            },
            {
                "answer": "The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag",
                "check": false
            },
            {
                "answer": "Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume",
                "check": false
            }
        ],
        "explanation": {
            "correct": "When an instance terminates, the value of the DeleteOnTermination attribute for each attached EBS volume determines whether to preserve or delete the volume. By default, the\nDeleteOnTermination attribute is set to True for the root volume and is set to False for all\nother volume types.<strong>Set the <code>DeleteOnTermination</code> attribute to False using the command line</strong> - If the instance is already running, you can set\n<code>DeleteOnTermination</code> to False using the command line.",
            "incorrect": "<strong>Update the attribute using AWS management console. Select the EC2 instance and then uncheck the Delete On Termination check box for the root EBS volume</strong> - You can\nset the <code>DeleteOnTermination</code> attribute to False when you launch a new instance.\nIt is not possible to update this attribute of a running instance from the AWS console.<strong>Set the <code>DisableApiTermination</code> attribute of the instance using the API</strong> - By default, you can terminate your instance using the Amazon EC2 console,\ncommand-line interface, or API. To prevent your instance from being accidentally terminated\nusing Amazon EC2, you can enable termination protection for the instance. The\n<code>DisableApiTermination</code> attribute controls whether the instance can be terminated\nusing the console, CLI, or API. This option cannot be used to control the delete status for\nthe EBS volume when the instance terminates.<strong>The attribute cannot be updated when the instance is running. Stop the instance from Amazon EC2 console and then update the flag</strong> - This statement is wrong and given\nonly as a distractor.References:",
            "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/terminating-instances.html#delete-on-termination-running-instance"
        }
    },
    {
        "id": 41,
        "questions": [
            "A junior developer has been asked to configure access to an Amazon EC2 instance hosting a web application. The developer has configured a new security group to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules. A custom Network Access Control List (NACL) connected with the instance's subnet is configured to permit incoming HTTP traffic from 0.0.0.0/0 and retained any default outbound rules.",
            "Which of the following solutions would you suggest if the EC2 instance needs to accept and respond to requests from the internet?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway",
                "check": false
            },
            {
                "answer": "The configuration is complete on the EC2 instance for accepting and responding to requests",
                "check": false
            },
            {
                "answer": "An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range",
                "check": true
            },
            {
                "answer": "An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>An outbound rule must be added to the Network ACL (NACL) to allow the response to be sent to the client on the ephemeral port range</strong>Security groups are stateful, so allowing inbound traffic to the necessary ports enables the connection. Network ACLs are stateless, so you must allow both inbound and outbound traffic.\nBy default, each custom Network ACL denies all inbound and outbound traffic until you add\nrules.To enable the connection to a service running on an instance, the associated network ACL must allow both:\n1. Inbound traffic on the port that the service is listening on\n2. Outbound traffic to ephemeral portsWhen a client connects to a server, a random port from the ephemeral port range (1024-65535) becomes the client's source port.The designated ephemeral port becomes the destination port for return traffic from the service. Outbound traffic to the ephemeral port must be allowed in the network ACL.",
            "incorrect": "<strong>The configuration is complete on the EC2 instance for accepting and responding to requests</strong> - As explained above, this is an incorrect statement.<strong>An outbound rule on the security group has to be configured, to allow the response to be sent to the client on the HTTP port</strong> - Security groups are stateful.\nTherefore you don't need a rule that allows responses to inbound traffic.<em>Outbound rules need to be configured both on the security group and on the NACL for sending responses to the Internet Gateway</em>* - Security Groups are stateful. Hence,\nreturn traffic is automatically allowed, so there is no need to configure an outbound rule\non the security group.References:",
            "reference": "https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports"
        }
    },
    {
        "id": 42,
        "questions": [
            "A pharmaceutical company uses Amazon EC2 instances for application hosting and Amazon CloudFront for content delivery. A new research paper with critical findings has to be shared with a research team that is spread across the world.",
            "Which of the following represents the most optimal solution to address this requirement without compromising the security of the content?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront",
                "check": false
            },
            {
                "answer": "Using CloudFront's Field-Level Encryption to help protect sensitive data",
                "check": false
            },
            {
                "answer": "Use CloudFront signed cookies feature to control access to the file",
                "check": false
            },
            {
                "answer": "Use CloudFront signed URL feature to control access to the file",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Use CloudFront signed URL feature to control access to the file</strong>A signed URL includes additional information, for example, expiration date and time, that gives you more control over access to your content.Here's an overview of how you configure CloudFront for signed URLs and how CloudFront responds when a user uses a signed URL to request a file:In your CloudFront distribution, specify one or more trusted key groups, which contain the public keys that CloudFront can use to verify the URL signature. You use\nthe corresponding private keys to sign the URLs.Develop your application to determine whether a user should have access to your content and to create signed URLs for the files or parts of your application that\nyou want to restrict access to.A user requests a file for which you want to require signed URLs. Your application verifies that the user is entitled to access the file: they've signed in, they've\npaid for access to the content, or they've met some other requirement for access.Your application creates and returns a signed URL to the user. The signed URL allows the user to download or stream the content.This step is automatic; the user usually doesn't have to do anything additional to access the content. For example, if a user is accessing your content in a web browser, your application\nreturns the signed URL to the browser. The browser immediately uses the signed URL to access\nthe file in the CloudFront edge cache without any intervention from the user.",
            "incorrect": "<strong>Use CloudFront signed cookies feature to control access to the file</strong> - CloudFront signed cookies allow you to control who can access your content when you don't\nwant to change your current URLs or when you want to provide access to multiple restricted\nfiles, for example, all of the files in the subscribers' area of a website. Our requirement\nhas only one file that needs to be shared and hence signed URL is the optimal solution.Signed URLs take precedence over signed cookies. If you use both signed URLs and signed cookies to control access to the same files and a viewer uses a signed URL to request a\nfile, CloudFront determines whether to return the file to the viewer based only on the\nsigned URL.<strong>Configure AWS Web Application Firewall (WAF) to monitor and control the HTTP and HTTPS requests that are forwarded to CloudFront</strong> - AWS WAF is a web application\nfirewall that lets you monitor the HTTP and HTTPS requests that are forwarded to CloudFront,\nand lets you control access to your content. Based on conditions that you specify, such as\nthe values of query strings or the IP addresses that requests originate from, CloudFront\nresponds to requests either with the requested content or with an HTTP status code 403\n(Forbidden). A firewall is optimal for broader use cases than restricted access to a single\nfile.<strong>Using CloudFront's Field-Level Encryption to help protect sensitive data</strong> - CloudFront's field-level encryption further encrypts sensitive data in an HTTPS form using\nfield-specific encryption keys (which you supply) before a POST request is forwarded to your\norigin. This ensures that sensitive data can only be decrypted and viewed by certain\ncomponents or services in your application stack. This feature is not useful for the given\nuse case.References:",
            "reference": "https://aws.amazon.com/about-aws/whats-new/2017/12/introducing-field-level-encryption-on-amazon-cloudfront/"
        }
    },
    {
        "id": 43,
        "questions": [
            "A banking application needs to send real-time alerts and notifications based on any updates from the backend services. The company wants to avoid implementing complex polling mechanisms for these notifications.",
            "Which of the following types of APIs supported by the Amazon API Gateway is the right fit?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "HTTP APIs",
                "check": false
            },
            {
                "answer": "WebSocket APIs",
                "check": true
            },
            {
                "answer": "REST APIs",
                "check": false
            },
            {
                "answer": "REST or HTTP APIs",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>WebSocket APIs</strong>In a WebSocket API, the client and the server can both send messages to each other at any time. Backend servers can easily push data to connected users and devices, avoiding the need\nto implement complex polling mechanisms.For example, you could build a serverless application using an API Gateway WebSocket API and AWS Lambda to send and receive messages to and from individual users or groups of users in a\nchat room. Or you could invoke backend services such as AWS Lambda, Amazon Kinesis, or an\nHTTP endpoint based on message content.You can use API Gateway WebSocket APIs to build secure, real-time communication applications without having to provision or manage any servers to manage connections or large-scale data\nexchanges. Targeted use cases include real-time applications such as the following:API Gateway provides WebSocket API management functionality such as the following:",
            "incorrect": "<strong>REST or HTTP APIs</strong><strong>REST APIs</strong> - An API Gateway REST API is made up of resources and methods. A resource is a logical entity that an app can access through a resource path. A method\ncorresponds to a REST API request that is submitted by the user of your API and the response\nreturned to the user.For example, /incomes could be the path of a resource representing the income of the app user. A resource can have one or more operations that are defined by appropriate HTTP verbs\nsuch as GET, POST, PUT, PATCH, and DELETE. A combination of a resource path and an operation\nidentifies a method of the API. For example, a POST /incomes method could add an income\nearned by the caller, and a GET /expenses method could query the reported expenses incurred\nby the caller.<strong>HTTP APIs</strong> - HTTP APIs enable you to create RESTful APIs with lower latency and lower cost than REST APIs. You can use HTTP APIs to send requests to AWS Lambda\nfunctions or to any publicly routable HTTP endpoint.For example, you can create an HTTP API that integrates with a Lambda function on the backend. When a client calls your API, API Gateway sends the request to the Lambda function\nand returns the function's response to the client.Server push mechanism is not possible in REST and HTTP APIs.Reference:",
            "reference": "https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overview-developer-experience.html"
        }
    },
    {
        "id": 44,
        "questions": [
            "AWS CloudFormation helps model and provision all the cloud infrastructure resources needed for your business.",
            "Which of the following services rely on CloudFormation to provision resources (Select two)?"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "AWS Lambda",
                "check": false
            },
            {
                "answer": "AWS CodeBuild",
                "check": false
            },
            {
                "answer": "AWS Serverless Application Model (AWS SAM)",
                "check": true
            },
            {
                "answer": "AWS Autoscaling",
                "check": false
            },
            {
                "answer": "AWS Elastic Beanstalk",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>AWS Elastic Beanstalk</strong> - AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services developed with Java, .NET, PHP, Node.js,\nPython, Ruby, Go, and Docker on familiar servers such as Apache, Nginx, Passenger, and IIS.\nElastic Beanstalk uses AWS CloudFormation to launch the resources in your environment and\npropagate configuration changes.<strong>AWS Serverless Application Model (AWS SAM)</strong> - You use the AWS SAM specification to define your serverless application. AWS SAM templates are an extension of\nAWS CloudFormation templates, with some additional components that make them easier to work\nwith. AWS SAM needs CloudFormation templates as a basis for its configuration.",
            "incorrect": "<strong>AWS Lambda</strong> - AWS Lambda lets you run code without provisioning or managing servers. You pay only for the compute time you consume. Hence, Lamda does not need\nCloudFormation to run its services.<strong>AWS Autoscaling</strong> - AWS Auto Scaling monitors your applications and automatically adjusts the capacity to maintain steady, predictable performance at the lowest\npossible cost. Using AWS Auto Scaling, it’s easy to setup application scaling for multiple\nresources across multiple services in minutes. Auto Scaling used CloudFormation but is not a\nmandatory requirement.<strong>AWS CodeBuild</strong> - AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready\nto deploy. With CodeBuild, you don’t need to provision, manage, and scale your own build\nservers. AWS CodePipeline uses AWS CloudFormation as a deployment action but is not a\nmandatory service.References:",
            "reference": "https://aws.amazon.com/elasticbeanstalk/"
        }
    },
    {
        "id": 45,
        "questions": [
            "You are getting ready for an event to show off your Alexa skill written in JavaScript. As you are testing your voice activation commands you find that some intents are not invoking as they should and you are struggling to figure out what is happening. You included the following code console.log(JSON.stringify(this.event)) in hopes of getting more details about the request to your Alexa skill.",
            "You would like the logs stored in an Amazon Simple Storage Service (S3) bucket named MyAlexaLog. How do you achieve this?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use CloudWatch integration feature with S3",
                "check": true
            },
            {
                "answer": "Use CloudWatch integration feature with Lambda",
                "check": false
            },
            {
                "answer": "Use CloudWatch integration feature with Kinesis",
                "check": false
            },
            {
                "answer": "Use CloudWatch integration feature with Glue",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Use CloudWatch integration feature with S3</strong>You can export log data from your CloudWatch log groups to an Amazon S3 bucket and use this data in custom processing and analysis, or to load onto other systems.",
            "incorrect": "<strong>Use CloudWatch integration feature with Kinesis</strong> - You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you\nconfigure the CloudWatch settings to send logs to S3.<strong>Use CloudWatch integration feature with Lambda</strong> - You can use both to do custom processing or analysis but with S3 you don't have to process anything. Instead, you\nconfigure the CloudWatch settings to send logs to S3.<strong>Use CloudWatch integration feature with Glue</strong> - AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and\nload their data for analytics. Glue is not the right fit for the given use-case.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/S3Export.html"
        }
    },
    {
        "id": 46,
        "questions": [
            "A large firm stores its static data assets on Amazon S3 buckets. Each service line of the firm has its own AWS account. For a business use case, the Finance department needs to give access to their S3 bucket's data to the Human Resources department.",
            "Which of the below options is NOT feasible for cross-account access of S3 bucket objects?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects",
                "check": false
            },
            {
                "answer": "Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects",
                "check": false
            },
            {
                "answer": "Use Cross-account IAM roles for programmatic and console access to S3 bucket objects",
                "check": false
            },
            {
                "answer": "Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only",
                "check": true
            }
        ],
        "explanation": {
            "correct": "<strong>Use IAM roles and resource-based policies delegate access across accounts within different partitions via programmatic access only</strong> - This statement is incorrect\nand hence the right choice for this question. IAM roles and resource-based policies delegate\naccess across accounts only within a single partition. For example, assume that you have an\naccount in US West (N. California) in the standard <code>aws</code> partition. You also have\nan account in China (Beijing) in the <code>aws-cn</code> partition. You can't use an Amazon\nS3 resource-based policy in your account in China (Beijing) to allow access for users in\nyour standard AWS account.",
            "incorrect": "<strong>Use Resource-based policies and AWS Identity and Access Management (IAM) policies for programmatic-only access to S3 bucket objects</strong> - Use bucket policies to manage\ncross-account control and audit the S3 object's permissions. If you apply a bucket policy at\nthe bucket level, you can define who can access (Principal element), which objects they can\naccess (Resource element), and how they can access (Action element). Applying a bucket\npolicy at the bucket level allows you to define granular access to different objects inside\nthe bucket by using multiple policies to control access. You can also review the bucket\npolicy to see who can access objects in an S3 bucket.<strong>Use Access Control List (ACL) and IAM policies for programmatic-only access to S3 bucket objects</strong> - Use object ACLs to manage permissions only for specific\nscenarios and only if ACLs meet your needs better than IAM and S3 bucket policies. Amazon S3\nACLs allow users to define only the following permissions sets: READ, WRITE, READ_ACP,\nWRITE_ACP, and FULL_CONTROL. You can use only an AWS account or one of the predefined Amazon\nS3 groups as a grantee for the Amazon S3 ACL.<strong>Use Cross-account IAM roles for programmatic and console access to S3 bucket objects</strong> - Not all AWS services support resource-based policies. This means that\nyou can use cross-account IAM roles to centralize permission management when providing\ncross-account access to multiple services. Using cross-account IAM roles simplifies\nprovisioning cross-account access to S3 objects that are stored in multiple S3 buckets,\nremoving the need to manage multiple policies for S3 buckets. This method allows\ncross-account access to objects that are owned or uploaded by another AWS account or AWS\nservices. If you don't use cross-account IAM roles, the object ACL must be modified.References:",
            "reference": "https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/"
        }
    },
    {
        "id": 47,
        "questions": [
            "A company stores confidential data on an Amazon Simple Storage Service (S3) bucket. New regulatory guidelines require that files be stored with server-side encryption. The encryption used must be Advanced Encryption Standard (AES-256) and the company does not want to manage S3 encryption keys.",
            "Which of the following options should you use?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "SSE-S3",
                "check": true
            },
            {
                "answer": "SSE-C",
                "check": false
            },
            {
                "answer": "Client Side Encryption",
                "check": false
            },
            {
                "answer": "SSE-KMS",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>SSE-S3</strong>Using Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3), each object is encrypted with a unique key employing strong multi-factor encryption. As an additional safeguard, it\nencrypts the key itself with a master key that it regularly rotates. Amazon S3 server-side\nencryption uses one of the strongest block ciphers available, 256-bit Advanced Encryption\nStandard (AES-256), to encrypt your data.",
            "incorrect": "<strong>SSE-C</strong> - You manage the encryption keys and Amazon S3 manages the encryption as it writes to disks and decryption when you access your objects.<strong>Client-Side Encryption</strong> - You can encrypt data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption\nkeys, and related tools.<strong>SSE-KMS</strong> - Similar to SSE-S3 and also provides you with an audit trail of when your key was used and by whom. Additionally, you have the option to create and manage\nencryption keys yourself.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingEncryption.html"
        }
    },
    {
        "id": 48,
        "questions": [
            "A company has more than 100 million members worldwide enjoying 125 million hours of TV shows and movies each day. The company uses AWS for nearly all its computing and storage needs, which use more than 10,000 server instances on AWS. This results in an extremely complex and dynamic networking environment where applications are constantly communicating inside AWS and across the Internet. Monitoring and optimizing its network is critical for the company.",
            "The company needs a solution for ingesting and analyzing the multiple terabytes of real-time data its network generates daily in the form of flow logs. Which technology/service should the company use to ingest this data economically and has the flexibility to direct this data to other downstream systems?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Amazon Kinesis Data Streams",
                "check": true
            },
            {
                "answer": "Amazon Simple Queue Service (SQS)",
                "check": false
            },
            {
                "answer": "Amazon Kinesis Firehose",
                "check": false
            },
            {
                "answer": "AWS Glue",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Amazon Kinesis Data Streams</strong>Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds\nof thousands of sources such as website clickstreams, database event streams, financial\ntransactions, social media feeds, IT logs, and location-tracking events. The data collected\nis available in milliseconds to enable real-time analytics use cases such as real-time\ndashboards, real-time anomaly detection, dynamic pricing, and more.Kinesis Data Streams enables real-time processing of streaming big data. It provides ordering of records, as well as the ability to read and/or replay records in the same order to\nmultiple Amazon Kinesis Applications. The Amazon Kinesis Client Library (KCL) delivers all\nrecords for a given partition key to the same record processor, making it easier to build\nmultiple applications reading from the same Amazon Kinesis data stream (for example, to\nperform counting, aggregation, and filtering).",
            "incorrect": "<strong>Amazon Simple Queue Service (SQS)</strong> - Amazon Simple Queue Service (Amazon SQS) offers a reliable, highly scalable hosted queue for storing messages as they travel between\ncomputers. Amazon SQS lets you easily move data between distributed application components\nand helps you build applications in which messages are processed independently (with\nmessage-level ack/fail semantics), such as automated workflows. AWS recommends using Amazon\nSQS for cases where individual message fail/success are important, message delays are needed\nand there is only one consumer for the messages received (if more than one consumers need to\nconsume the message, then AWS suggests configuring more queues).<strong>Amazon Kinesis Firehose</strong> - Amazon Kinesis Data Firehose is the easiest way to load streaming data into data stores and analytics tools. It can capture, transform, and\nload streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and\nSplunk, enabling near real-time analytics with existing business intelligence tools and\ndashboards you’re already using today. It is a fully managed service that automatically\nscales to match the throughput of your data and requires no ongoing administration.Kinesis data streams is highly customizable and best suited for developers building custom applications or streaming data for specialized needs. Data Streams also provide greater\nflexibility in integrating downstream applications than Firehose. Data Streams is also a\ncost-effective option compared to Firehose. Therefore, KDS is the right solution.<strong>AWS Glue</strong> - AWS Glue is a serverless data integration service that makes it easy to discover, prepare, and combine data for analytics, machine learning, and application\ndevelopment. AWS Glue provides all of the capabilities needed for data integration so that\nyou can start analyzing your data and putting it to use in minutes instead of months. Glue\nis not best suited to handle real-time data.References:",
            "reference": "https://aws.amazon.com/kinesis/data-firehose/"
        }
    },
    {
        "id": 49,
        "questions": [
            "You are storing your video files in a separate S3 bucket than your main static website in an S3 bucket. When accessing the video URLs directly the users can view the videos on the browser, but they can't play the videos while visiting the main website.",
            "What is the root cause of this problem?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Amend the IAM policy",
                "check": false
            },
            {
                "answer": "Enable CORS",
                "check": true
            },
            {
                "answer": "Disable Server-Side Encryption",
                "check": false
            },
            {
                "answer": "Change the bucket policy",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Enable CORS</strong>Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support,\nyou can build rich client-side web applications with Amazon S3 and selectively allow\ncross-origin access to your Amazon S3 resources.To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access\nyour bucket, the operations (HTTP methods) that will support for each origin, and other\noperation-specific information.For the given use-case, you would create a <code>&lt;CORSRule&gt;</code> in <code>&lt;CORSConfiguration&gt;</code> for bucket B to allow access from the S3 website\norigin hosted on bucket A.",
            "incorrect": "<strong>Change the bucket policy</strong> - A bucket policy is a resource-based AWS Identity and Access Management (IAM) policy that grants permissions. With this policy, you can do\nthings such as allow one IP address to access the video file in the S3 bucket. In this\nscenario, we know that's not the case because it works using the direct URL but it doesn't\nwork when you click on a link to access the video.<strong>Amend the IAM policy</strong> - You attach IAM policies to IAM users, groups, or roles, which are then subject to the permissions you've defined. This scenario refers to\npublic users of a website and they need not have an IAM user account.<strong>Disable Server-Side Encryption</strong> - Amazon S3 encrypts your data at the object level as it writes it to disks in its data centers and decrypts it for you when you access\nit, if the video file is encrypted at rest then there is nothing you need to do because AWS\nhandles encrypt and decrypt. Disabling encryption is not an issue because you can access the\nvideo directly using an URL but not from the main website.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html"
        }
    },
    {
        "id": 50,
        "questions": [
            "DevOps engineers are developing an order processing system where notifications are sent to a department whenever an order is placed for a product. The system also pushes identical notifications of the new order to a processing module that would allow EC2 instances to handle the fulfillment of the order. In the case of processing errors, the messages should be allowed to be re-processed at a later stage. The order processing system should be able to scale transparently without the need for any manual or programmatic provisioning of resources.",
            "Which of the following solutions can be used to address this use-case in the most cost-efficient way?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "SNS + SQS",
                "check": true
            },
            {
                "answer": "SNS + Lambda",
                "check": false
            },
            {
                "answer": "SQS + SES",
                "check": false
            },
            {
                "answer": "SNS + Kinesis",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>SNS + SQS</strong>Amazon SNS enables message filtering and fanout to a large number of subscribers, including serverless functions, queues, and distributed systems. Additionally, Amazon SNS fans out\nnotifications to end users via mobile push messages, SMS, and email.Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables you to decouple and scale microservices, distributed systems, and serverless applications. SQS\noffers two types of message queues. Standard queues offer maximum throughput, best-effort\nordering, and at-least-once delivery. SQS FIFO queues are designed to guarantee that\nmessages are processed exactly once, in the exact order that they are sent.Because each buffered request can be processed independently, Amazon SQS can scale transparently to handle the load without any provisioning instructions from you.SNS and SQS can be used to create a fanout messaging scenario in which messages are \"pushed\" to multiple subscribers, which eliminates the need to periodically check or poll for updates\nand enables parallel asynchronous processing of the message by the subscribers. SQS can\nallow for later re-processing and dead letter queues. This is called the fan-out pattern.",
            "incorrect": "<strong>SNS + Kinesis</strong> - You can use Amazon Kinesis Data Streams to collect and process large streams of data records in real-time. Kinesis Data Streams stores records from\n24 hours (by default) to 8760 hours (365 days). However, you need to manually provision\nshards in case the load increases or you need to use CloudWatch alarms to set up auto\nscaling for the shards. Since Kinesis only supports transparent scaling in the on-demand\nmode, however, it is not cost efficient for the given use case, so this option is not the\nright fit for the given use case.<strong>SNS + Lambda</strong> - Amazon SNS and AWS Lambda are integrated so you can invoke Lambda functions with Amazon SNS notifications. The Lambda function receives the message\npayload as an input parameter and can manipulate the information in the message, publish the\nmessage to other SNS topics, or send the message to other AWS services. However, your EC2\ninstances cannot \"poll\" from Lambda functions and as such, this would not work.<strong>SQS + SES</strong> - This will not work as the messages need to be processed twice (once for sending the notification and later for order fulfillment) and SQS only allows for\none consuming application.References:",
            "reference": "https://aws.amazon.com/getting-started/tutorials/send-fanout-event-notifications/"
        }
    },
    {
        "id": 51,
        "questions": [
            "A company uses Amazon Simple Email Service (SES) to cost-effectively send susbscription emails to the customers. Intermittently, the SES service throws the error: Throttling – Maximum sending rate exceeded.",
            "As a developer associate, which of the following would you recommend to fix this issue?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again",
                "check": true
            },
            {
                "answer": "Raise a service request with Amazon to increase the throttling limit for the SES API",
                "check": false
            },
            {
                "answer": "Configure Timeout mechanism for each request made to the SES service",
                "check": false
            },
            {
                "answer": "Implement retry mechanism for all 4xx errors to avoid throttling error",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Use Exponential Backoff technique to introduce delay in time before attempting to execute the operation again</strong> - A “Throttling – Maximum sending rate exceeded”\nerror is retriable. This error is different than other errors returned by Amazon SES. A\nrequest rejected with a “Throttling” error can be retried at a later time and is likely to\nsucceed.Retries are “selfish.” In other words, when a client retries, it spends more of the server's time to get a higher chance of success. Where failures are rare or transient, that's not a\nproblem. This is because the overall number of retried requests is small, and the tradeoff\nof increasing apparent availability works well. When failures are caused by overload,\nretries that increase load can make matters significantly worse. They can even delay\nrecovery by keeping the load high long after the original issue is resolved.The preferred solution is to use a backoff. Instead of retrying immediately and aggressively, the client waits some amount of time between tries. The most common pattern is an\nexponential backoff, where the wait time is increased exponentially after every attempt.A variety of factors can affect your send rate, e.g. message size, network performance or Amazon SES availability. The advantage of the exponential backoff approach is that your\napplication will self-tune and it will call Amazon SES at close to the maximum allowed rate.",
            "incorrect": "<strong>Configure Timeout mechanism for each request made to the SES service</strong> - Requests are configured to timeout if they do not complete successfully in a given time.\nThis helps free up the database, application and any other resource that could potentially\nkeep on waiting to eventually succeed. But, if errors are caused by load, retries can be\nineffective if all clients retry at the same time. Throttling error signifies that load is\nhigh on SES and it does not make sense to keep retrying.<strong>Raise a service request with Amazon to increase the throttling limit for the SES API</strong> - If throttling error is persistent, then it indicates a high load on the\nsystem consistently and increasing the throttling limit will be the right solution for the\nproblem. But, the error is only intermittent here, signifying that decreasing the rate of\nrequests will handle the error.<strong>Implement retry mechanism for all 4xx errors to avoid throttling error</strong> - 4xx status codes indicate that there was a problem with the client request. Common client\nrequest errors include providing invalid credentials and omitting required parameters. When\nyou get a 4xx error, you need to correct the problem and resubmit a properly formed client\nrequest. Throttling is a server error and not a client error, hence retry on 4xx errors does\nnot make sense here.References:",
            "reference": "https://aws.amazon.com/blogs/messaging-and-targeting/how-to-handle-a-throttling-maximum-sending-rate-exceeded-error/"
        }
    },
    {
        "id": 52,
        "questions": [
            "A firm runs its technology operations on a fleet of Amazon EC2 instances. The firm needs a certain software to be available on the instances to support their daily workflows. The developer team has been told to use the user data feature of EC2 instances.",
            "Which of the following are true about the user data EC2 configuration? ( Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "By default, user data runs only during the boot cycle when you first launch an instance",
                "check": true
            },
            {
                "answer": "By default, scripts entered as user data are executed with root user privileges",
                "check": true
            },
            {
                "answer": "By default, scripts entered as user data do not have root user privileges for executing",
                "check": false
            },
            {
                "answer": "When an instance is running, you can update user data by using root user credentials",
                "check": false
            },
            {
                "answer": "By default, user data is executed every time an EC2 instance is re-started",
                "check": false
            }
        ],
        "explanation": {
            "correct": "",
            "incorrect": "<strong>By default, user data is executed every time an EC2 instance is re-started</strong> - As discussed above, this is not a default configuration of the system. But, can be achieved\nby explicitly configuring the instance.<strong>When an instance is running, you can update user data by using root user credentials</strong> - You can't change the user data if the instance is running (even\nby using root user credentials), but you can view it.<strong>By default, scripts entered as user data do not have root user privileges for executing</strong> - Scripts entered as user data are executed as the root user, hence\ndo not need the sudo command in the script.Reference:",
            "reference": "https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html"
        }
    },
    {
        "id": 53,
        "questions": [
            "A developer is defining the signers that can create signed URLs for their Amazon CloudFront distributions.",
            "Which of the following statements should the developer consider while defining the signers? (Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs",
                "check": false
            },
            {
                "answer": "When you create a signer, the public key is with CloudFront and private key is used to sign a portion of URL",
                "check": true
            },
            {
                "answer": "CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources",
                "check": false
            },
            {
                "answer": "You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs",
                "check": false
            },
            {
                "answer": "When you use the root user to manage CloudFront key pairs, you can only have up to two active CloudFront key pairs per AWS account",
                "check": true
            }
        ],
        "explanation": {
            "correct": "",
            "incorrect": "<strong>You can also use AWS Identity and Access Management (IAM) permissions policies to restrict what the root user can do with CloudFront key pairs</strong> - When you use the\nAWS account root user to manage CloudFront key pairs, you can’t restrict what the root user\ncan do or the conditions in which it can do them. You can’t apply IAM permissions policies\nto the root user, which is one reason why AWS best practices recommend against using the\nroot user.<strong>CloudFront key pairs can be created with any account that has administrative permissions and full access to CloudFront resources</strong> - CloudFront key pairs can\nonly be created using the root user account and hence is not a best practice to create\nCloudFront key pairs as signers.<strong>Both the signers (trusted key groups and CloudFront key pairs) can be managed using the CloudFront APIs</strong> - With CloudFront key groups, you can manage public keys,\nkey groups, and trusted signers using the CloudFront API. You can use the API to automate\nkey creation and key rotation. When you use the AWS root user, you have to use the AWS\nManagement Console to manage CloudFront key pairs, so you can’t automate the process.Reference:",
            "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-content-trusted-signers.html"
        }
    },
    {
        "id": 54,
        "questions": [
            "Your company has embraced cloud-native microservices architectures. New applications must be dockerized and stored in a registry service offered by AWS. The architecture should support dynamic port mapping and support multiple tasks from a single service on the same container instance. All services should run on the same EC2 instance.",
            "Which of the following options offers the best-fit solution for the given use-case?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Application Load Balancer + Beanstalk",
                "check": false
            },
            {
                "answer": "Classic Load Balancer + ECS",
                "check": false
            },
            {
                "answer": "Application Load Balancer + ECS",
                "check": true
            },
            {
                "answer": "Classic Load Balancer + Beanstalk",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Application Load Balancer + ECS</strong>Amazon Elastic Container Service (Amazon ECS) is a highly scalable, fast, container management service that makes it easy to run, stop, and manage Docker containers on a\ncluster. You can host your cluster on a serverless infrastructure that is managed by Amazon\nECS by launching your services or tasks using the Fargate launch type. For more control over\nyour infrastructure, you can host your tasks on a cluster of Amazon Elastic Compute Cloud\n(Amazon EC2) instances that you manage by using the EC2 launch type.An Application load balancer distributes incoming application traffic across multiple targets, such as EC2 instances, in multiple Availability Zones. A listener checks for\nconnection requests from clients, using the protocol and port that you configure. The rules\nthat you define for a listener determine how the load balancer routes requests to its\nregistered targets. Each rule consists of a priority, one or more actions, and one or more\nconditions.When you deploy your services using Amazon Elastic Container Service (Amazon ECS), you can use dynamic port mapping to support multiple tasks from a single service on the same\ncontainer instance. Amazon ECS manages updates to your services by automatically registering\nand deregistering containers with your target group using the instance ID and port for each\ncontainer.",
            "incorrect": "<strong>Classic Load Balancer + Beanstalk</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task on the same instance. Instead, with the Classic Load\nBalancer, you must statically map port numbers on a container instance. So this option is\nruled out.<strong>Application Load Balancer + Beanstalk</strong> - You can create docker environments that support multiple containers per Amazon EC2 instance with a multi-container Docker\nplatform for Elastic Beanstalk. However, ECS gives you finer control.<strong>Classic Load Balancer + ECS</strong> - The Classic Load Balancer doesn't allow you to run multiple copies of a task in the same instance. Instead, with the Classic Load Balancer,\nyou must statically map port numbers on a container instance. So this option is ruled out.References:",
            "reference": "https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/create_deploy_docker_ecs.html"
        }
    },
    {
        "id": 55,
        "questions": [
            "A development team at a social media company uses AWS Lambda functions for its serverless stack on AWS Cloud. For a new deployment, the Team Lead wants to send only a certain portion of the traffic to a new version of a Lambda function. In case the deployment goes wrong, the solution should also support the ability to roll back to a previous version of the Lambda function, with MIMINUM downtime for the application.",
            "As a Developer Associate, which of the following options would you recommend to address this use-case?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Set up the application to have multiple alias of the Lambda function. Deploy the new version of the code. Configure a new alias that points to the current alias of the\nLambda function for handling 10% of the traffic. If the deployment goes wrong, reset the\nnew alias to point all traffic to the most recent working alias of the Lambda function",
                "check": false
            },
            {
                "answer": "Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure the alias to send 10% of the users to this new\nversion. If the deployment goes wrong, reset the alias to point all traffic to the\ncurrent version",
                "check": true
            },
            {
                "answer": "Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure alias to send all users to this new version. If the\ndeployment goes wrong, reset the alias to point to the current version",
                "check": false
            },
            {
                "answer": "Set up the application to directly deploy the new Lambda version. If the deployment goes wrong, reset the application back to the current version using the version number in the\nARN",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure the alias to send 10% of the users to this new\nversion. If the deployment goes wrong, reset the alias to point all traffic to the\ncurrent version</strong>You can use versions to manage the deployment of your AWS Lambda functions. For example, you can publish a new version of a function for beta testing without affecting users of the\nstable production version. You can change the function code and settings only on the\nunpublished version of a function. When you publish a version, the code and most of the\nsettings are locked to ensure a consistent experience for users of that version.You can create one or more aliases for your AWS Lambda function. A Lambda alias is like a pointer to a specific Lambda function version. You can use routing configuration on an alias\nto send a portion of traffic to a Lambda function version. For example, you can reduce the\nrisk of deploying a new version by configuring the alias to send most of the traffic to the\nexisting version, and only a small percentage of traffic to the new version.",
            "incorrect": "<strong>Set up the application to use an alias that points to the current version. Deploy the new version of the code and configure alias to send all users to this new version. If\nthe deployment goes wrong, reset the alias to point to the current version</strong> - In\nthis case, the application uses an alias to send all traffic to the new version which does\nnot meet the requirement of sending only a certain portion of the traffic to the new Lambda\nversion. In addition, if the deployment goes wrong, the application would see a downtime.\nHence this option is incorrect.<strong>Set up the application to directly deploy the new Lambda version. If the deployment goes wrong, reset the application back to the current version using the version number\nin the ARN</strong> - In this case, the application sends all traffic to the new version\nwhich does not meet the requirement of sending only a certain portion of the traffic to the\nnew Lambda version. In addition, if the deployment goes wrong, the application would see a\ndowntime. Hence this option is incorrect.<strong>Set up the application to have multiple alias of the Lambda function. Deploy the new version of the code. Configure a new alias that points to the current alias of the\nLambda function for handling 10% of the traffic. If the deployment goes wrong, reset the\nnew alias to point all traffic to the most recent working alias of the Lambda\nfunction</strong> - This option has been added as a distractor. The alias for a Lambda\nfunction can only point to a Lambda function version. It cannot point to another alias.References:",
            "reference": "https://docs.aws.amazon.com/lambda/latest/dg/configuration-versions.html"
        }
    },
    {
        "id": 56,
        "questions": [
            "A company would like to migrate the existing application code from a GitHub repository to AWS CodeCommit.",
            "As an AWS Certified Developer Associate, which of the following would you recommend for migrating the cloned repository to CodeCommit over HTTPS?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use IAM user secret access key and access key ID",
                "check": false
            },
            {
                "answer": "Use IAM Multi-Factor authentication",
                "check": false
            },
            {
                "answer": "Use Git credentials generated from IAM",
                "check": true
            },
            {
                "answer": "Use authentication offered by GitHub secure tokens",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Use Git credentials generated from IAM</strong> - CodeCommit repositories are Git-based and support the basic functionalities of Git such as Git credentials. AWS\nrecommends that you use an IAM user when working with CodeCommit. You can access CodeCommit\nwith other identity types, but the other identity types are subject to limitations.The simplest way to set up connections to AWS CodeCommit repositories is to configure Git credentials for CodeCommit in the IAM console, and then use those credentials for HTTPS\nconnections. You can also use these same credentials with any third-party tool or individual\ndevelopment environment (IDE) that supports HTTPS authentication using a static user name\nand password.An IAM user is an identity within your Amazon Web Services account that has specific custom permissions. For example, an IAM user can have permissions to create and manage Git\ncredentials for accessing CodeCommit repositories. This is the recommended user type for\nworking with CodeCommit. You can use an IAM user name and password to sign in to secure AWS\nwebpages like the AWS Management Console, AWS Discussion Forums, or the AWS Support Center.",
            "incorrect": "<strong>Use IAM Multi-Factor authentication</strong> - AWS Multi-Factor Authentication (MFA) is a simple best practice that adds an extra layer of protection on top of your user name\nand password. With MFA enabled, when a user signs in to an AWS Management Console, they will\nbe prompted for their user name and password (the first factor—what they know), as well as\nfor an authentication code from their AWS MFA device (the second factor—what they have).\nTaken together, these multiple factors provide increased security for your AWS account\nsettings and resources.<strong>Use IAM user secret access key and access key ID</strong> - Access keys are long-term credentials for an IAM user or the AWS account root user. You can use access keys to sign\nprogrammatic requests to the AWS CLI or AWS API (directly or using the AWS SDK). As a best\npractice, AWS suggests using temporary security credentials (IAM roles) instead of access\nkeys.<strong>Use authentication offered by GitHub secure tokens</strong> - Personal access tokens (PATs) are an alternative to using passwords for authentication to GitHub when using the\nGitHub API or the command line. This option is specific to GitHub only and hence not useful\nfor the given use case.References:",
            "reference": "https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html"
        }
    },
    {
        "id": 57,
        "questions": [
            "Your company leverages Amazon CloudFront to provide content via the internet to customers with low latency. Aside from latency, security is another concern and you are looking for help in enforcing end-to-end connections using HTTPS so that content is protected.",
            "Which of the following options is available for HTTPS in AWS CloudFront?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Between clients and CloudFront as well as between CloudFront and backend",
                "check": true
            },
            {
                "answer": "Between clients and CloudFront only",
                "check": false
            },
            {
                "answer": "Neither between clients and CloudFront nor between CloudFront and backend",
                "check": false
            },
            {
                "answer": "Between CloudFront and backend only",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Between clients and CloudFront as well as between CloudFront and backend</strong>For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so connections are encrypted when CloudFront communicates with\nviewers.You also can configure CloudFront to use HTTPS to get objects from your origin, so connections are encrypted when CloudFront communicates with your origin.",
            "incorrect": "<strong>Between clients and CloudFront only</strong> - This is incorrect as you can choose to require HTTPS between CloudFront and your origin.<strong>Between CloudFront and backend only</strong> - This is incorrect as you can choose to require HTTPS between viewers and CloudFront.<strong>Neither between clients and CloudFront nor between CloudFront and backend</strong> - This is incorrect as you can choose HTTPS settings both for communication between viewers\nand CloudFront as well as between CloudFront and your origin.References:",
            "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html"
        }
    },
    {
        "id": 58,
        "questions": [
            "ECS Fargate container tasks are usually spread across Availability Zones (AZs) and the underlying workloads need persistent cross-AZ shared access to the data volumes configured for the container tasks.",
            "Which of the following solutions is the best choice for these workloads?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Amazon EFS volumes",
                "check": true
            },
            {
                "answer": "AWS Gateway Storage volumes",
                "check": false
            },
            {
                "answer": "Docker volumes",
                "check": false
            },
            {
                "answer": "Bind mounts",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Amazon EFS volumes</strong> - EFS volumes provide a simple, scalable, and persistent file storage for use with your Amazon ECS tasks. With Amazon EFS, storage capacity is\nelastic, growing and shrinking automatically as you add and remove files. Your applications\ncan have the storage they need, when they need it. Amazon EFS volumes are supported for\ntasks hosted on Fargate or Amazon EC2 instances.You can use Amazon EFS file systems with Amazon ECS to export file system data across your fleet of container instances. That way, your tasks have access to the same persistent\nstorage, no matter the instance on which they land. However, you must configure your\ncontainer instance AMI to mount the Amazon EFS file system before the Docker daemon starts.\nAlso, your task definitions must reference volume mounts on the container instance to use\nthe file system.",
            "incorrect": "<strong>Docker volumes</strong> - A Docker-managed volume that is created under /var/lib/docker/volumes on the host Amazon EC2 instance. Docker volume drivers (also\nreferred to as plugins) are used to integrate the volumes with external storage systems,\nsuch as Amazon EBS. The built-in local volume driver or a third-party volume driver can be\nused. Docker volumes are only supported when running tasks on Amazon EC2 instances.<strong>Bind mounts</strong> - A file or directory on the host, such as an Amazon EC2 instance or AWS Fargate, is mounted into a container. Bind mount host volumes are supported\nfor tasks hosted on Fargate or Amazon EC2 instances. Bind mounts provide temporary storage,\nand hence these are a wrong choice for this use case.<strong>AWS Storage Gateway volumes</strong> - This is an incorrect choice, given only as a distractor.Reference:",
            "reference": "https://aws.amazon.com/blogs/containers/amazon-ecs-availability-best-practices/"
        }
    },
    {
        "id": 59,
        "questions": [
            "What steps can a developer take to optimize the performance of a CPU-bound AWS Lambda function and ensure fast response time?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Increase the function's CPU",
                "check": false
            },
            {
                "answer": "Increase the function's timeout",
                "check": false
            },
            {
                "answer": "Increase the function's memory",
                "check": true
            },
            {
                "answer": "Increase the function's provisioned concurrency",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Increase the function's memory</strong>Memory is the principal lever available to Lambda developers for controlling the performance of a function. You can configure the amount of memory allocated to a Lambda function,\nbetween 128 MB and 10,240 MB. The Lambda console defaults new functions to the smallest\nsetting and many developers also choose 128 MB for their functions.The amount of memory also determines the amount of virtual CPU available to a function. Adding more memory proportionally increases the amount of CPU, increasing the overall\ncomputational power available. If a function is CPU-, network- or memory-bound, then\nchanging the memory setting can dramatically improve its performance.",
            "incorrect": "<strong>Increase the function's provisioned concurrency</strong><strong>Increase the function's reserved concurrency</strong>In Lambda, concurrency is the number of requests your function can handle at the same time. There are two types of concurrency controls available:Reserved concurrency – Reserved concurrency guarantees the maximum number of concurrent instances for the function. When a function has reserved concurrency, no other function can\nuse that concurrency. There is no charge for configuring reserved concurrency for a\nfunction.Provisioned concurrency – Provisioned concurrency initializes a requested number of execution environments so that they are prepared to respond immediately to your function's\ninvocations. Note that configuring provisioned concurrency incurs charges to your AWS\naccount.Neither reserved concurrency nor provisioned concurrency has any impact on the CPU available to a function, so both these options are incorrect<strong>Increase the function's CPU</strong> - This is a distractor as you cannot directly increase the CPU available to a function.References:",
            "reference": "https://docs.aws.amazon.com/lambda/latest/dg/provisioned-concurrency.html"
        }
    },
    {
        "id": 60,
        "questions": [
            "An e-commerce company has multiple EC2 instances operating in a private subnet which is part of a custom VPC. These instances are running an image processing application that needs to access images stored on S3. Once each image is processed, the status of the corresponding record needs to be marked as completed in a DynamoDB table.",
            "How would you go about providing private access to these AWS resources which are not part of this custom VPC?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using the\nprivate IP address",
                "check": false
            },
            {
                "answer": "Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC",
                "check": true
            },
            {
                "answer": "Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address",
                "check": false
            },
            {
                "answer": "Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB service\nusing the private IP address",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Create a separate gateway endpoint for S3 and DynamoDB each. Add two new target entries for these two gateway endpoints in the route table of the custom VPC</strong>Endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between instances in your VPC and services without\nimposing availability risks or bandwidth constraints on your network traffic.A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink without requiring an internet gateway, NAT\ndevice, VPN connection, or AWS Direct Connect connection. Instances in your VPC do not\nrequire public IP addresses to communicate with resources in the service. Traffic between\nyour VPC and the other service does not leave the Amazon network.There are two types of VPC endpoints: interface endpoints and gateway endpoints. An interface endpoint is an elastic network interface with a private IP address from the IP address range\nof your subnet that serves as an entry point for traffic destined to a supported service.A gateway endpoint is a gateway that you specify as a target for a route in your route table for traffic destined to a supported AWS service. The following AWS services are supported:Amazon S3DynamoDBYou should note that S3 now supports both gateway endpoints as well as the interface endpoints.",
            "incorrect": "<strong>Create a gateway endpoint for S3 and add it as a target in the route table of the custom VPC. Create an interface endpoint for DynamoDB and then connect to the DynamoDB\nservice using the private IP address</strong><strong>Create a separate interface endpoint for S3 and DynamoDB each. Then connect to these services using the private IP address</strong>DynamoDB does not support interface endpoints, so these two options are incorrect.<strong>Create a gateway endpoint for DynamoDB and add it as a target in the route table of the custom VPC. Create an API endpoint for S3 and then connect to the S3 service using\nthe private IP address</strong> - There is no such thing as an API endpoint for S3. API\nendpoints are used with AWS API Gateway. This option has been added as a distractor.References:",
            "reference": "https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html"
        }
    },
    {
        "id": 61,
        "questions": [
            "A company is looking at storing their less frequently accessed files on AWS that can be concurrently accessed by hundreds of EC2 instances. The company needs the most cost-effective file storage service that provides immediate access to data whenever needed.",
            "Which of the following options represents the best solution for the given requirements?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Amazon Elastic File System (EFS) Standard–IA storage class",
                "check": true
            },
            {
                "answer": "Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class",
                "check": false
            },
            {
                "answer": "Amazon Elastic Block Store (EBS)",
                "check": false
            },
            {
                "answer": "Amazon Elastic File System (EFS) Standard storage class",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Amazon Elastic File System (EFS) Standard–IA storage class</strong> - Amazon EFS is a file storage service for use with Amazon compute (EC2, containers, serverless) and\non-premises servers. Amazon EFS provides a file system interface, file system access\nsemantics (such as strong consistency and file locking), and concurrently accessible storage\nfor up to thousands of Amazon EC2 instances.The Standard–IA storage class reduces storage costs for files that are not accessed every day. It does this without sacrificing the high availability, high durability, elasticity,\nand POSIX file system access that Amazon EFS provides. AWS recommends Standard-IA storage if\nyou need your full dataset to be readily accessible and want to automatically save on\nstorage costs for files that are less frequently accessed.",
            "incorrect": "<strong>Amazon S3 Standard-Infrequent Access (S3 Standard-IA) storage class</strong> - Amazon S3 is an object storage service. Amazon S3 makes data available through an Internet API that\ncan be accessed anywhere. It is not a file storage service, as is needed in the use case.<strong>Amazon Elastic File System (EFS) Standard storage class</strong> - Amazon EFS Standard storage classes are ideal for workloads that require the highest levels of\ndurability and availability. The EFS Standard storage class is used for frequently accessed\nfiles. It is the storage class to which customer data is initially written for Standard\nstorage classes. The company is also looking at cutting costs by optimally storing the\ninfrequently accessed data. Hence, EFS standard storage class is not the right solution for\nthe given use case.<strong>Amazon Elastic Block Store (EBS)</strong> - Amazon EBS is a block-level storage service for use with Amazon EC2. Amazon EBS can deliver performance for workloads that\nrequire the lowest latency access to data from a single EC2 instance. EBS volume cannot be\naccessed by hundreds of EC2 instances concurrently. It is not a file storage service, as is\nneeded in the use case.Reference:",
            "reference": "https://docs.aws.amazon.com/efs/latest/ug/storage-classes.html"
        }
    },
    {
        "id": 62,
        "questions": [
            "A serverless application built on AWS processes customer orders 24/7 using an AWS Lambda function and communicates with an external vendor's HTTP API for payment processing. The development team wants to notify the support team in near real-time using an existing Amazon Simple Notification Service (Amazon SNS) topic, but only when the external API error rate exceeds 5% of the total transactions processed in an hour.",
            "As an AWS Certified Developer Associate, which option will you suggest as the most efficient solution?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to\ncheck the output from CloudWatch Metric Filter on a schedule and send notification via\nthe existing SNS topic when the error rate exceeds the specified rate",
                "check": false
            },
            {
                "answer": "Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to\ncheck the output from CloudWatch Logs Insights on a schedule and send notification via\nthe existing SNS topic when the error rate exceeds the specified rate",
                "check": false
            },
            {
                "answer": "Configure and push high-resolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that sends a\nnotification via the existing SNS topic when the error rate exceeds the specified rate",
                "check": true
            },
            {
                "answer": "Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the existing SNS\ntopic when the error rate exceeds the specified rate",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Configure and push high-resolution custom metrics to CloudWatch that record the failures of the external payment processing API calls. Create a CloudWatch alarm that\nsends a notification via the existing SNS topic when the error rate exceeds the\nspecified rate</strong>You can publish your own metrics, known as custom metrics, to CloudWatch using the AWS CLI or an API.Each metric is one of the following:Standard resolution, with data having a one-minute granularityHigh resolution, with data at a granularity of one secondMetrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you\npublish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and\nyou can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds,\nor any multiple of 60 seconds.High-resolution metrics can give you more immediate insight into your application's sub-minute activity. Keep in mind that every PutMetricData call for a custom metric is\ncharged, so calling PutMetricData more often on a high-resolution metric can lead to higher\ncharges.You can create metric and composite alarms in Amazon CloudWatch. For the given use case, you can set up a CloudWatch metric alarm that watches the custom metric that captures the API\nerrors and then triggers the alarm when the API error rate exceeds the 5% threshold. The\nalarm then sends a notification via the existing SNS topic.",
            "incorrect": "<strong>Configure CloudWatch metrics with detailed monitoring for the external payment processing API calls. Create a CloudWatch alarm that sends a notification via the\nexisting SNS topic when the error rate exceeds the specified rate</strong> - CloudWatch\nprovides two categories of monitoring: basic monitoring and detailed monitoring. Detailed\nmonitoring options differ based on the services that offer it. For example, Amazon EC2\ndetailed monitoring provides more frequent metrics, published at one-minute intervals,\ninstead of the five-minute intervals used in Amazon EC2 basic monitoring. Detailed\nmonitoring is offered by only some services. As explained above, you need to use custom\nmetrics to capture data for the external payment processing API calls since detailed\nmonitoring for the standard CloudWatch metrics cannot be used for this scenario.<strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Logs Insights to query the CloudWatch logs. Set up the Lambda function to\ncheck the output from CloudWatch Logs Insights on a schedule and send notification via\nthe existing SNS topic when the error rate exceeds the specified rate</strong> -\nCloudWatch Logs Insights enables you to interactively search and analyze your log data in\nAmazon CloudWatch Logs. You can perform queries to help you more efficiently and effectively\nrespond to operational issues. This option is not the right fit for the given use case since\nLambda cannot monitor the output of the CloudWatch Logs Insights on a real-time basis since\nit is being invoked on a schedule. Also, it is not an efficient solution since Lambda will\nneed significant custom code to parse and compute the external API error rate from the\nCloudWatch Logs Insights data.<strong>Log the results of payment processing API calls to Amazon CloudWatch. Leverage Amazon CloudWatch Metric Filter to look at the CloudWatch logs. Set up the Lambda function to\ncheck the output from CloudWatch Metric Filter on a schedule and send notification via\nthe existing SNS topic when the error rate exceeds the specified rate</strong> - You can\nsearch and filter the log data coming into CloudWatch Logs by creating one or more metric\nfilters. Metric filters define the terms and patterns to look for in log data as it is sent\nto CloudWatch Logs. CloudWatch Logs uses these metric filters to turn log data into\nnumerical CloudWatch metrics that you can graph or set an alarm on. This option is not the\nbest fit for the given use case since Lambda cannot monitor the output of the CloudWatch\nMetric Filter on a real-time basis since it is being invoked on a schedule. Also, it is not\nan efficient solution since Lambda will need significant custom code to parse and compute\nthe external API error rate from the CloudWatch Metric Filter data.References:",
            "reference": "https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/MonitoringLogData.html"
        }
    },
    {
        "id": 63,
        "questions": [
            "A development team wants to build an application using serverless architecture. The team plans to use AWS Lambda functions extensively to achieve this goal. The developers of the team work on different programming languages like Python, .NET and Javascript. The team wants to model the cloud infrastructure using any of these programming languages.",
            "Which AWS service/tool should the team use for the given use-case?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "AWS CodeDeploy",
                "check": false
            },
            {
                "answer": "AWS Cloud Development Kit (CDK)",
                "check": true
            },
            {
                "answer": "AWS CloudFormation",
                "check": false
            },
            {
                "answer": "AWS Serverless Application Model (SAM)",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>AWS Cloud Development Kit (CDK)</strong> - The AWS Cloud Development Kit (AWS CDK) is an open-source software development framework to define your cloud application resources\nusing familiar programming languages.Provisioning cloud applications can be a challenging process that requires you to perform manual actions, write custom scripts, maintain templates, or learn domain-specific\nlanguages. AWS CDK uses the familiarity and expressive power of programming languages such\nas JavaScript/TypeScript, Python, Java, and .NET for modeling your applications. It provides\nyou with high-level components called constructs that preconfigure cloud resources with\nproven defaults, so you can build cloud applications without needing to be an expert. AWS\nCDK provisions your resources in a safe, repeatable manner through AWS CloudFormation. It\nalso enables you to compose and share your own custom constructs that incorporate your\norganization's requirements, helping you start new projects faster.",
            "incorrect": "<strong>AWS CloudFormation</strong> - When AWS CDK applications are run, they compile down to fully formed CloudFormation JSON/YAML templates that are then submitted to the\nCloudFormation service for provisioning. Because the AWS CDK leverages CloudFormation, you\nstill enjoy all the benefits CloudFormation provides such as safe deployment, automatic\nrollback, and drift detection. But, CloudFormation by itself is not sufficient for the\ncurrent use case.<strong>AWS Serverless Application Model (SAM)</strong> - The AWS Serverless Application Repository is a managed repository for serverless applications. It enables teams,\norganizations, and individual developers to store and share reusable applications, and\neasily assemble and deploy serverless architectures in powerful new ways. Using the\nServerless Application Repository, you don't need to clone, build, package, or publish\nsource code to AWS before deploying it.AWS Serverless Application Model and AWS CDK both abstract AWS infrastructure as code making it easier for you to define your cloud infrastructure. If you prefer defining your\nserverless infrastructure in concise declarative templates, SAM is the better fit. If you\nwant to define your AWS infrastructure in a familiar programming language, as is the\nrequirement in the current use case, AWS CDK is the right fit.<strong>AWS CodeDeploy</strong> - AWS CodeDeploy is a fully managed deployment service that automates software deployments to a variety of compute services such as Amazon EC2, AWS\nFargate, AWS Lambda, and your on-premises servers. AWS CodeDeploy makes it easier for you to\nrapidly release new features, helps you avoid downtime during application deployment, and\nhandles the complexity of updating your applications. CodeDeploy can be used with AWS CDK\nfor deployments.Reference:",
            "reference": "https://aws.amazon.com/cdk/faqs/"
        }
    },
    {
        "id": 64,
        "questions": [
            "You have migrated an on-premise SQL Server database to an Amazon Relational Database Service (RDS) database attached to a VPC inside a private subnet. Also, the related Java application, hosted on-premise, has been moved to an Amazon Lambda function.",
            "Which of the following should you implement to connect AWS Lambda function to its RDS instance?"
        ],
        "single_choice": true,
        "answers": [
            {
                "answer": "Use Lambda layers to connect to the internet and RDS separately",
                "check": false
            },
            {
                "answer": "Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS",
                "check": true
            },
            {
                "answer": "Use Environment variables to pass in the RDS connection string",
                "check": false
            },
            {
                "answer": "Configure lambda to connect to the public subnet that will give internet access and use Security Group to access RDS inside the private subnet",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Configure Lambda to connect to VPC with private subnet and Security Group needed to access RDS</strong> - You can configure a Lambda function to connect to private subnets\nin a virtual private cloud (VPC) in your account. Use Amazon Virtual Private Cloud (Amazon\nVPC) to create a private network for resources such as databases, cache instances, or\ninternal services. Connect your lambda function to the VPC to access private resources\nduring execution. When you connect a function to a VPC, Lambda creates an elastic network\ninterface for each combination of the security group and subnet in your function's VPC\nconfiguration. This is the right way of giving RDS access to Lambda.",
            "incorrect": "<strong>Use Lambda layers to connect to the internet and RDS separately</strong> - You can configure your Lambda function to pull in additional code and content in the form of layers.\nA layer is a ZIP archive that contains libraries, a custom runtime, or other dependencies.\nLayers will not help in configuring access to RDS instance and hence is an incorrect choice.<strong>Configure lambda to connect to the public subnet that will give internet access and use the Security Group to access RDS inside the private subnet</strong> - This is an\nincorrect statement. Connecting a Lambda function to a public subnet does not give it\ninternet access or a public IP address. To grant internet access to your function, its\nassociated VPC must have a NAT gateway (or NAT instance) in a public subnet.<strong>Use Environment variables to pass in the RDS connection string</strong> - You can use environment variables to store secrets securely and adjust your function's behavior without\nupdating code. You can use environment variables to exchange data with RDS, but you will\nstill need access to RDS, which is not possible with just environment variables.References:",
            "reference": "https://aws.amazon.com/premiumsupport/knowledge-center/internet-access-lambda-function/"
        }
    },
    {
        "id": 65,
        "questions": [
            "A business hosts its website on Amazon EC2 instances and employs Auto Scaling to adjust its resources according to traffic spikes. However, users globally report slow loading times because static content hosted on the EC2 instances takes too long to load, even outside of busy periods.",
            "What pair of actions should be taken to improve the latency of the website? (Select two)"
        ],
        "single_choice": false,
        "answers": [
            {
                "answer": "Transfer the application’s static content hosted on EC2 instances to Amazon S3",
                "check": true
            },
            {
                "answer": "Migrate the application to AWS Lambda",
                "check": false
            },
            {
                "answer": "Set up an Amazon CloudFront distribution to cache the static content with Amazon S3 configured as the origin",
                "check": true
            },
            {
                "answer": "Upgrade the CPU and RAM available to the EC2 instances",
                "check": false
            },
            {
                "answer": "Double the Auto Scaling group’s desired capacity",
                "check": false
            }
        ],
        "explanation": {
            "correct": "<strong>Set up an Amazon CloudFront distribution to cache the static content with Amazon S3 configured as the origin</strong><strong>Transfer the application’s static content hosted on EC2 instances to Amazon S3</strong>Amazon CloudFront is a web service that speeds up distribution of your static and dynamic web content, such as .html, .css, .js, and image files, to your users. CloudFront delivers your\ncontent through a worldwide network of data centers called edge locations. When a user\nrequests content that you're serving with CloudFront, the request is routed to the edge\nlocation that provides the lowest latency (time delay), so that content is delivered with\nthe best possible performance.For the given use case, you can transfer the static content from EC2 instances to Amazon S3. Then, you can specify the origin as the Amazon S3 bucket from which CloudFront gets your\nfiles which will then be distributed from CloudFront edge locations all over the world. An\norigin stores the original, definitive version of your objects. If you're serving content\nover HTTP, your origin is either an Amazon S3 bucket or an HTTP server, such as a web\nserver.",
            "incorrect": "<strong>Upgrade the CPU and RAM available to the EC2 instances</strong> - Since the static content takes too long to load even outside of busy periods, this implies that the\nunderlying root cause is the high end-to-end network latency rather than the hardware of the\nEC2 instance.<strong>Double the Auto Scaling group’s desired capacity</strong> - The desired capacity represents the initial capacity of the Auto Scaling group at the time of creation. An Auto\nScaling group attempts to maintain the desired capacity. It starts by launching the number\nof instances that are specified for the desired capacity, and maintains this number of\ninstances as long as there are no scaling policies or scheduled actions attached to the Auto\nScaling group. Since the static content takes too long to load even outside of busy periods,\nso doubling the desired capacity would still not address the underlying root cause of high\nend-to-end network latency.<strong>Migrate the application to AWS Lambda</strong> - You cannot store static content on AWS Lambda, so this option just serves as a distractor.References:",
            "reference": "https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/Introduction.html"
        }
    }
]